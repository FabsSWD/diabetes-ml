{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "138c71cb-7ffe-4ad6-82db-d4e1c0347e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar las bibliotecas necesarias\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36aeb7fd-817b-409c-b809-dbf204a2979d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset\n",
    "doc = \"diabetes.csv\"\n",
    "data = pd.read_csv(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "261abc03-495f-4942-8c8e-03169e933b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostrar las primeras filas del conjunto de datos para inspección inicial\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "223d7241-1789-47bd-96b3-2681ed9ab2b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pregnancies                 0\n",
       "Glucose                     0\n",
       "BloodPressure               0\n",
       "SkinThickness               0\n",
       "Insulin                     0\n",
       "BMI                         0\n",
       "DiabetesPedigreeFunction    0\n",
       "Age                         0\n",
       "Outcome                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificar la existencia de valores faltantes en el conjunto de datos\n",
    "missing_values = data.isnull().sum()\n",
    "missing_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21584526-a952-44fd-8193-a447ca83c749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Glucose            5\n",
       "BloodPressure     35\n",
       "SkinThickness    227\n",
       "Insulin          374\n",
       "BMI               11\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Contar los valores de cero en columnas donde estos podrían indicar datos faltantes\n",
    "potential_missing_zero_counts = data[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']].eq(0).sum()\n",
    "potential_missing_zero_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05e13bf0-f0bb-4edf-ac31-b74059b0f4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USUARIO\\AppData\\Local\\Temp\\ipykernel_51504\\3438395538.py:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[column].replace(0, median_value, inplace=True)\n",
      "C:\\Users\\USUARIO\\AppData\\Local\\Temp\\ipykernel_51504\\3438395538.py:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[column].replace(0, median_value, inplace=True)\n",
      "C:\\Users\\USUARIO\\AppData\\Local\\Temp\\ipykernel_51504\\3438395538.py:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[column].replace(0, median_value, inplace=True)\n",
      "C:\\Users\\USUARIO\\AppData\\Local\\Temp\\ipykernel_51504\\3438395538.py:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[column].replace(0, median_value, inplace=True)\n",
      "C:\\Users\\USUARIO\\AppData\\Local\\Temp\\ipykernel_51504\\3438395538.py:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[column].replace(0, median_value, inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>125</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>125</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>29</td>\n",
       "      <td>125</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35      125  33.6   \n",
       "1            1       85             66             29      125  26.6   \n",
       "2            8      183             64             29      125  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imputación de valores cero con la mediana de cada columna\n",
    "for column in ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']:\n",
    "    median_value = data[column][data[column] != 0].median()\n",
    "    data[column].replace(0, median_value, inplace=True)\n",
    "\n",
    "# Mostrar los primeros datos para verificar la imputación\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34b39ca8-714d-4e1a-a249-438fb327f10c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Outcome\n",
       "0    500\n",
       "1    268\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Verificar el balance de clases en la columna 'Outcome'\n",
    "class_counts = data['Outcome'].value_counts()\n",
    "\n",
    "# Visualización de la distribución de clases\n",
    "plt.figure(figsize=(6, 4))\n",
    "class_counts.plot(kind='bar', color=['skyblue', 'salmon'])\n",
    "plt.title('Distribución de Clases en el Conjunto de Datos (Outcome)')\n",
    "plt.xlabel('Outcome')\n",
    "plt.ylabel('Cantidad de Ejemplos')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n",
    "\n",
    "class_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "029f3074-0d5a-43af-ba9f-10ace04b69a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m logreg_weighted \u001b[38;5;241m=\u001b[39m LogisticRegression(max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Entrenar el modelo con ponderación de clases\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m logreg_weighted\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train\u001b[49m, y_train)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Predecir en el conjunto de validación\u001b[39;00m\n\u001b[0;32m      8\u001b[0m val_predictions_weighted \u001b[38;5;241m=\u001b[39m logreg_weighted\u001b[38;5;241m.\u001b[39mpredict(X_val)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Aplicar ponderación de clases en el modelo de regresión logística\n",
    "logreg_weighted = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')\n",
    "\n",
    "# Entrenar el modelo con ponderación de clases\n",
    "logreg_weighted.fit(X_train, y_train)\n",
    "\n",
    "# Predecir en el conjunto de validación\n",
    "val_predictions_weighted = logreg_weighted.predict(X_val)\n",
    "\n",
    "# Evaluar el rendimiento del modelo ponderado\n",
    "val_accuracy_weighted = accuracy_score(y_val, val_predictions_weighted)\n",
    "val_precision_weighted = precision_score(y_val, val_predictions_weighted)\n",
    "val_recall_weighted = recall_score(y_val, val_predictions_weighted)\n",
    "val_confusion_matrix_weighted = confusion_matrix(y_val, val_predictions_weighted)\n",
    "\n",
    "val_accuracy_weighted, val_precision_weighted, val_recall_weighted, val_confusion_matrix_weighted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fb46a33e-c397-46d3-b29d-f60e80baebd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7217391304347827,\n",
       " 0.5869565217391305,\n",
       " 0.675,\n",
       " array([[56, 19],\n",
       "        [13, 27]], dtype=int64))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Aplicar sobremuestreo de la clase minoritaria\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_train_oversampled, y_train_oversampled = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# Entrenar el modelo de regresión logística en los datos sobremuestreados\n",
    "logreg_oversampled = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logreg_oversampled.fit(X_train_oversampled, y_train_oversampled)\n",
    "\n",
    "# Predecir en el conjunto de validación\n",
    "val_predictions_oversampled = logreg_oversampled.predict(X_val)\n",
    "\n",
    "# Evaluar el rendimiento del modelo sobremuestreado\n",
    "val_accuracy_oversampled = accuracy_score(y_val, val_predictions_oversampled)\n",
    "val_precision_oversampled = precision_score(y_val, val_predictions_oversampled)\n",
    "val_recall_oversampled = recall_score(y_val, val_predictions_oversampled)\n",
    "val_confusion_matrix_oversampled = confusion_matrix(y_val, val_predictions_oversampled)\n",
    "\n",
    "val_accuracy_oversampled, val_precision_oversampled, val_recall_oversampled, val_confusion_matrix_oversampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "06ecacef-9433-4d26-9308-f90fd153ff05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    66\n",
       "1    49\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualización de la distribución de clases después del ajuste de ponderación\n",
    "# Distribución de las predicciones del modelo ponderado en el conjunto de validación\n",
    "\n",
    "# Contar las predicciones de cada clase en el conjunto de validación\n",
    "weighted_class_counts = pd.Series(val_predictions_weighted).value_counts()\n",
    "\n",
    "# Graficar la distribución de predicciones del modelo ponderado\n",
    "plt.figure(figsize=(6, 4))\n",
    "weighted_class_counts.plot(kind='bar', color=['skyblue', 'salmon'])\n",
    "plt.title('Distribución de Predicciones en el Conjunto de Validación (Modelo Ponderado)')\n",
    "plt.xlabel('Clase Predicha')\n",
    "plt.ylabel('Cantidad de Ejemplos')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n",
    "\n",
    "weighted_class_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3d75759-8522-4c78-a715-48b99c5d4d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.69921875, 0.14973958333333334, 0.15104166666666666)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# División del conjunto de datos\n",
    "train_data, temp_data = train_test_split(data, test_size=0.3, random_state=42, stratify=data['Outcome'])\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42, stratify=temp_data['Outcome'])\n",
    "\n",
    "# Verificar las proporciones de cada conjunto\n",
    "train_prop = len(train_data) / len(data)\n",
    "val_prop = len(val_data) / len(data)\n",
    "test_prop = len(test_data) / len(data)\n",
    "\n",
    "train_prop, val_prop, test_prop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b4ea003d-b2bb-41e6-a0a9-e458499acee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7130434782608696,\n",
       " 0.5945945945945946,\n",
       " 0.55,\n",
       " array([[60, 15],\n",
       "        [18, 22]], dtype=int64))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Separar características y etiquetas\n",
    "X_train, y_train = train_data.drop(columns=['Outcome']), train_data['Outcome']\n",
    "X_val, y_val = val_data.drop(columns=['Outcome']), val_data['Outcome']\n",
    "X_test, y_test = test_data.drop(columns=['Outcome']), test_data['Outcome']\n",
    "\n",
    "# Definir el modelo de regresión logística y los hiperparámetros para búsqueda en GridSearch\n",
    "logreg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "param_grid = {'C': [0.1, 1, 10, 100], 'solver': ['lbfgs', 'liblinear']}\n",
    "grid_search = GridSearchCV(logreg, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Entrenar el modelo utilizando GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtener el mejor modelo\n",
    "best_logreg = grid_search.best_estimator_\n",
    "val_predictions = best_logreg.predict(X_val)\n",
    "\n",
    "# Evaluar el rendimiento en el conjunto de validación\n",
    "val_accuracy = accuracy_score(y_val, val_predictions)\n",
    "val_precision = precision_score(y_val, val_predictions)\n",
    "val_recall = recall_score(y_val, val_predictions)\n",
    "val_confusion_matrix = confusion_matrix(y_val, val_predictions)\n",
    "\n",
    "val_accuracy, val_precision, val_recall, val_confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d185d807-ddd9-47bd-bf7a-286324285e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7931034482758621,\n",
       " 0.717948717948718,\n",
       " 0.6829268292682927,\n",
       " array([[64, 11],\n",
       "        [13, 28]], dtype=int64))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aplicar el modelo ponderado al conjunto de prueba\n",
    "test_predictions_weighted = logreg_weighted.predict(X_test)\n",
    "\n",
    "# Evaluar el rendimiento del modelo ponderado en el conjunto de prueba\n",
    "test_accuracy_weighted = accuracy_score(y_test, test_predictions_weighted)\n",
    "test_precision_weighted = precision_score(y_test, test_predictions_weighted)\n",
    "test_recall_weighted = recall_score(y_test, test_predictions_weighted)\n",
    "test_confusion_matrix_weighted = confusion_matrix(y_test, test_predictions_weighted)\n",
    "\n",
    "test_accuracy_weighted, test_precision_weighted, test_recall_weighted, test_confusion_matrix_weighted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13bffb0-15b2-4534-88d2-1b45fe623911",
   "metadata": {},
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ad3022-4401-445d-9b83-ada12f104c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "251ed6c4-7ee5-4bf8-9169-b3a91b8987aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dereck.marin\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7844827771186829"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Preparar los datos de entrada y salida para Keras\n",
    "X_train_keras = np.array(X_train)\n",
    "y_train_keras = np.array(y_train)\n",
    "X_val_keras = np.array(X_val)\n",
    "y_val_keras = np.array(y_val)\n",
    "X_test_keras = np.array(X_test)\n",
    "y_test_keras = np.array(y_test)\n",
    "\n",
    "# Calcular la ponderación de clases\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_keras), y=y_train_keras)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# Definir el modelo de red neuronal en Keras\n",
    "model = Sequential([\n",
    "    Dense(16, input_shape=(X_train_keras.shape[1],), activation='relu'),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), \n",
    "              loss=BinaryCrossentropy(), \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Entrenar el modelo con ponderación de clases\n",
    "history = model.fit(X_train_keras, y_train_keras, \n",
    "                    epochs=100, \n",
    "                    batch_size=16, \n",
    "                    validation_data=(X_val_keras, y_val_keras), \n",
    "                    class_weight=class_weight_dict, \n",
    "                    verbose=0)\n",
    "\n",
    "# Evaluar el modelo en el conjunto de prueba\n",
    "test_loss, test_accuracy = model.evaluate(X_test_keras, y_test_keras, verbose=0)\n",
    "test_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "996d48eb-9bb2-4f76-be8f-1c5782b8478d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dereck.marin\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3869 - loss: 9.0873Epoch 1: Loss = 5.5827, Accuracy = 0.4451, Val_Loss = 4.0778, Val_Accuracy = 0.5043\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.4058 - loss: 8.1232 - val_accuracy: 0.5043 - val_loss: 4.0778\n",
      "Epoch 2/240\n",
      "\u001b[1m28/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6163 - loss: 1.3358Epoch 2: Loss = 1.1154, Accuracy = 0.6238, Val_Loss = 1.8522, Val_Accuracy = 0.5565\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6169 - loss: 1.2978 - val_accuracy: 0.5565 - val_loss: 1.8522\n",
      "Epoch 3/240\n",
      "\u001b[1m26/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6733 - loss: 0.9471Epoch 3: Loss = 0.8109, Accuracy = 0.6667, Val_Loss = 1.2754, Val_Accuracy = 0.5826\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6708 - loss: 0.9176 - val_accuracy: 0.5826 - val_loss: 1.2754\n",
      "Epoch 4/240\n",
      "\u001b[1m25/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6033 - loss: 0.9855Epoch 4: Loss = 0.7387, Accuracy = 0.6592, Val_Loss = 0.9764, Val_Accuracy = 0.6261\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6163 - loss: 0.9235 - val_accuracy: 0.6261 - val_loss: 0.9764\n",
      "Epoch 5/240\n",
      "\u001b[1m26/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6555 - loss: 0.6710 Epoch 5: Loss = 0.6903, Accuracy = 0.6518, Val_Loss = 0.8409, Val_Accuracy = 0.6000\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6525 - loss: 0.6737 - val_accuracy: 0.6000 - val_loss: 0.8409\n",
      "Epoch 6/240\n",
      "\u001b[1m24/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6706 - loss: 0.6548Epoch 6: Loss = 0.6759, Accuracy = 0.6592, Val_Loss = 0.7420, Val_Accuracy = 0.6261\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6678 - loss: 0.6608 - val_accuracy: 0.6261 - val_loss: 0.7420\n",
      "Epoch 7/240\n",
      "\u001b[1m32/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6093 - loss: 0.6687Epoch 7: Loss = 0.6681, Accuracy = 0.6238, Val_Loss = 0.7169, Val_Accuracy = 0.6435\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6105 - loss: 0.6687 - val_accuracy: 0.6435 - val_loss: 0.7169\n",
      "Epoch 8/240\n",
      "\u001b[1m26/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6174 - loss: 0.6736Epoch 8: Loss = 0.6669, Accuracy = 0.6648, Val_Loss = 0.7054, Val_Accuracy = 0.6261\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6296 - loss: 0.6728 - val_accuracy: 0.6261 - val_loss: 0.7054\n",
      "Epoch 9/240\n",
      "\u001b[1m29/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6273 - loss: 0.6673Epoch 9: Loss = 0.6622, Accuracy = 0.6331, Val_Loss = 0.7115, Val_Accuracy = 0.6261\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6279 - loss: 0.6668 - val_accuracy: 0.6261 - val_loss: 0.7115\n",
      "Epoch 10/240\n",
      "\u001b[1m18/34\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6496 - loss: 0.6506Epoch 10: Loss = 0.6632, Accuracy = 0.6648, Val_Loss = 0.7183, Val_Accuracy = 0.6261\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6540 - loss: 0.6591 - val_accuracy: 0.6261 - val_loss: 0.7183\n",
      "Epoch 11/240\n",
      "\u001b[1m27/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6493 - loss: 0.6525Epoch 11: Loss = 0.6606, Accuracy = 0.6443, Val_Loss = 0.7003, Val_Accuracy = 0.6261\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6470 - loss: 0.6547 - val_accuracy: 0.6261 - val_loss: 0.7003\n",
      "Epoch 12/240\n",
      "\u001b[1m31/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6613 - loss: 0.6528Epoch 12: Loss = 0.6598, Accuracy = 0.6555, Val_Loss = 0.6853, Val_Accuracy = 0.6261\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6608 - loss: 0.6535 - val_accuracy: 0.6261 - val_loss: 0.6853\n",
      "Epoch 13/240\n",
      "\u001b[1m27/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6521 - loss: 0.6816Epoch 13: Loss = 0.6582, Accuracy = 0.6462, Val_Loss = 0.6672, Val_Accuracy = 0.6609\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6519 - loss: 0.6768 - val_accuracy: 0.6609 - val_loss: 0.6672\n",
      "Epoch 14/240\n",
      "\u001b[1m24/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6377 - loss: 0.6642Epoch 14: Loss = 0.6495, Accuracy = 0.6536, Val_Loss = 0.6803, Val_Accuracy = 0.6609\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6412 - loss: 0.6611 - val_accuracy: 0.6609 - val_loss: 0.6803\n",
      "Epoch 15/240\n",
      "\u001b[1m30/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6266 - loss: 0.6457Epoch 15: Loss = 0.6440, Accuracy = 0.6536, Val_Loss = 0.6687, Val_Accuracy = 0.6435\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6319 - loss: 0.6454 - val_accuracy: 0.6435 - val_loss: 0.6687\n",
      "Epoch 16/240\n",
      "\u001b[1m26/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6704 - loss: 0.6354Epoch 16: Loss = 0.6420, Accuracy = 0.6499, Val_Loss = 0.6670, Val_Accuracy = 0.6522\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6643 - loss: 0.6372 - val_accuracy: 0.6522 - val_loss: 0.6670\n",
      "Epoch 17/240\n",
      "\u001b[1m26/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6691 - loss: 0.6455Epoch 17: Loss = 0.6411, Accuracy = 0.6704, Val_Loss = 0.6684, Val_Accuracy = 0.6609\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6697 - loss: 0.6446 - val_accuracy: 0.6609 - val_loss: 0.6684\n",
      "Epoch 18/240\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6590 - loss: 0.6444Epoch 18: Loss = 0.6389, Accuracy = 0.6704, Val_Loss = 0.6596, Val_Accuracy = 0.6783\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6593 - loss: 0.6442 - val_accuracy: 0.6783 - val_loss: 0.6596\n",
      "Epoch 19/240\n",
      "\u001b[1m26/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6662 - loss: 0.6496Epoch 19: Loss = 0.6375, Accuracy = 0.6778, Val_Loss = 0.6708, Val_Accuracy = 0.6870\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6711 - loss: 0.6463 - val_accuracy: 0.6870 - val_loss: 0.6708\n",
      "Epoch 20/240\n",
      "\u001b[1m27/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6268 - loss: 0.6589Epoch 20: Loss = 0.6381, Accuracy = 0.6834, Val_Loss = 0.6659, Val_Accuracy = 0.6870\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6386 - loss: 0.6550 - val_accuracy: 0.6870 - val_loss: 0.6659\n",
      "Epoch 21/240\n",
      "\u001b[1m27/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6692 - loss: 0.6176Epoch 21: Loss = 0.6384, Accuracy = 0.6611, Val_Loss = 0.6639, Val_Accuracy = 0.6870\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6680 - loss: 0.6211 - val_accuracy: 0.6870 - val_loss: 0.6639\n",
      "Epoch 22/240\n",
      "\u001b[1m33/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6611 - loss: 0.6431Epoch 22: Loss = 0.6367, Accuracy = 0.6685, Val_Loss = 0.6571, Val_Accuracy = 0.6783\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6615 - loss: 0.6429 - val_accuracy: 0.6783 - val_loss: 0.6571\n",
      "Epoch 23/240\n",
      "\u001b[1m30/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6665 - loss: 0.6515Epoch 23: Loss = 0.6381, Accuracy = 0.6909, Val_Loss = 0.6712, Val_Accuracy = 0.7043\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6708 - loss: 0.6492 - val_accuracy: 0.7043 - val_loss: 0.6712\n",
      "Epoch 24/240\n",
      "\u001b[1m27/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6342 - loss: 0.6412Epoch 24: Loss = 0.6370, Accuracy = 0.6685, Val_Loss = 0.6528, Val_Accuracy = 0.6870\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6413 - loss: 0.6407 - val_accuracy: 0.6870 - val_loss: 0.6528\n",
      "Epoch 25/240\n",
      "\u001b[1m30/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6493 - loss: 0.6467Epoch 25: Loss = 0.6337, Accuracy = 0.6760, Val_Loss = 0.6580, Val_Accuracy = 0.6957\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6534 - loss: 0.6456 - val_accuracy: 0.6957 - val_loss: 0.6580\n",
      "Epoch 26/240\n",
      "\u001b[1m29/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6608 - loss: 0.6418Epoch 26: Loss = 0.6349, Accuracy = 0.6723, Val_Loss = 0.6457, Val_Accuracy = 0.6696\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6617 - loss: 0.6412 - val_accuracy: 0.6696 - val_loss: 0.6457\n",
      "Epoch 27/240\n",
      "\u001b[1m33/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6936 - loss: 0.6410Epoch 27: Loss = 0.6333, Accuracy = 0.6890, Val_Loss = 0.6533, Val_Accuracy = 0.6870\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6934 - loss: 0.6408 - val_accuracy: 0.6870 - val_loss: 0.6533\n",
      "Epoch 28/240\n",
      "\u001b[1m18/34\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6114 - loss: 0.6724Epoch 28: Loss = 0.6355, Accuracy = 0.6816, Val_Loss = 0.6479, Val_Accuracy = 0.6783\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6353 - loss: 0.6587 - val_accuracy: 0.6783 - val_loss: 0.6479\n",
      "Epoch 29/240\n",
      "\u001b[1m29/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6507 - loss: 0.6443Epoch 29: Loss = 0.6367, Accuracy = 0.6350, Val_Loss = 0.6499, Val_Accuracy = 0.6870\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6493 - loss: 0.6428 - val_accuracy: 0.6870 - val_loss: 0.6499\n",
      "Epoch 30/240\n",
      "\u001b[1m31/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6781 - loss: 0.6453Epoch 30: Loss = 0.6331, Accuracy = 0.6909, Val_Loss = 0.6460, Val_Accuracy = 0.6783\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6795 - loss: 0.6442 - val_accuracy: 0.6783 - val_loss: 0.6460\n",
      "Epoch 31/240\n",
      "\u001b[1m27/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6710 - loss: 0.6261Epoch 31: Loss = 0.6306, Accuracy = 0.6741, Val_Loss = 0.6543, Val_Accuracy = 0.7043\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6709 - loss: 0.6276 - val_accuracy: 0.7043 - val_loss: 0.6543\n",
      "Epoch 32/240\n",
      "\u001b[1m26/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6344 - loss: 0.6542Epoch 32: Loss = 0.6308, Accuracy = 0.6872, Val_Loss = 0.6507, Val_Accuracy = 0.7043\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6462 - loss: 0.6507 - val_accuracy: 0.7043 - val_loss: 0.6507\n",
      "Epoch 33/240\n",
      "\u001b[1m33/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6684 - loss: 0.6287Epoch 33: Loss = 0.6291, Accuracy = 0.6853, Val_Loss = 0.6579, Val_Accuracy = 0.7043\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6693 - loss: 0.6287 - val_accuracy: 0.7043 - val_loss: 0.6579\n",
      "Epoch 34/240\n",
      "\u001b[1m 1/34\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.6875 - loss: 0.6465 0.6326Epoch 34: Loss = 0.6296, Accuracy = 0.6536, Val_Loss = 0.6668, Val_Accuracy = 0.6870\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6658 - loss: 0.6176 - val_accuracy: 0.6870 - val_loss: 0.6668\n",
      "Epoch 35/240\n",
      "\u001b[1m32/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6289 - loss: 0.6494Epoch 35: Loss = 0.6281, Accuracy = 0.6667, Val_Loss = 0.6685, Val_Accuracy = 0.6957\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6322 - loss: 0.6481 - val_accuracy: 0.6957 - val_loss: 0.6685\n",
      "Epoch 36/240\n",
      "\u001b[1m29/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5869 - loss: 0.6426Epoch 36: Loss = 0.6267, Accuracy = 0.6350, Val_Loss = 0.6604, Val_Accuracy = 0.7043\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5936 - loss: 0.6405 - val_accuracy: 0.7043 - val_loss: 0.6604\n",
      "Epoch 37/240\n",
      "\u001b[1m32/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6510 - loss: 0.6168Epoch 37: Loss = 0.6257, Accuracy = 0.6592, Val_Loss = 0.6471, Val_Accuracy = 0.6870\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6517 - loss: 0.6176 - val_accuracy: 0.6870 - val_loss: 0.6471\n",
      "Epoch 38/240\n",
      "\u001b[1m33/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6624 - loss: 0.6262Epoch 38: Loss = 0.6220, Accuracy = 0.6555, Val_Loss = 0.6501, Val_Accuracy = 0.6957\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6620 - loss: 0.6260 - val_accuracy: 0.6957 - val_loss: 0.6501\n",
      "Epoch 39/240\n",
      "\u001b[1m28/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6594 - loss: 0.6402Epoch 39: Loss = 0.6223, Accuracy = 0.6648, Val_Loss = 0.6335, Val_Accuracy = 0.6783\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6608 - loss: 0.6356 - val_accuracy: 0.6783 - val_loss: 0.6335\n",
      "Epoch 40/240\n",
      "\u001b[1m28/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6888 - loss: 0.6137Epoch 40: Loss = 0.6209, Accuracy = 0.6555, Val_Loss = 0.6603, Val_Accuracy = 0.6609\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6819 - loss: 0.6148 - val_accuracy: 0.6609 - val_loss: 0.6603\n",
      "Epoch 41/240\n",
      "\u001b[1m30/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6576 - loss: 0.6229Epoch 41: Loss = 0.6206, Accuracy = 0.6704, Val_Loss = 0.6624, Val_Accuracy = 0.6522\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6589 - loss: 0.6229 - val_accuracy: 0.6522 - val_loss: 0.6624\n",
      "Epoch 42/240\n",
      "\u001b[1m28/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6188 - loss: 0.6275Epoch 42: Loss = 0.6189, Accuracy = 0.6555, Val_Loss = 0.6546, Val_Accuracy = 0.6696\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6254 - loss: 0.6262 - val_accuracy: 0.6696 - val_loss: 0.6546\n",
      "Epoch 43/240\n",
      "\u001b[1m28/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6755 - loss: 0.6040Epoch 43: Loss = 0.6190, Accuracy = 0.6909, Val_Loss = 0.6572, Val_Accuracy = 0.6783\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6785 - loss: 0.6064 - val_accuracy: 0.6783 - val_loss: 0.6572\n",
      "Epoch 44/240\n",
      "\u001b[1m28/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6721 - loss: 0.6015 Epoch 44: Loss = 0.6205, Accuracy = 0.6350, Val_Loss = 0.6234, Val_Accuracy = 0.6609\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6621 - loss: 0.6064 - val_accuracy: 0.6609 - val_loss: 0.6234\n",
      "Epoch 45/240\n",
      "\u001b[1m30/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7338 - loss: 0.6068Epoch 45: Loss = 0.6175, Accuracy = 0.7132, Val_Loss = 0.6941, Val_Accuracy = 0.5826\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7311 - loss: 0.6082 - val_accuracy: 0.5826 - val_loss: 0.6941\n",
      "Epoch 46/240\n",
      "\u001b[1m32/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6231 - loss: 0.6165Epoch 46: Loss = 0.6204, Accuracy = 0.6406, Val_Loss = 0.6517, Val_Accuracy = 0.6522\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6248 - loss: 0.6166 - val_accuracy: 0.6522 - val_loss: 0.6517\n",
      "Epoch 47/240\n",
      "\u001b[1m33/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7151 - loss: 0.6024Epoch 47: Loss = 0.6182, Accuracy = 0.6927, Val_Loss = 0.6471, Val_Accuracy = 0.6609\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7138 - loss: 0.6029 - val_accuracy: 0.6609 - val_loss: 0.6471\n",
      "Epoch 48/240\n",
      "\u001b[1m 4/34\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6745 - loss: 0.5917Epoch 48: Loss = 0.6130, Accuracy = 0.6667, Val_Loss = 0.6192, Val_Accuracy = 0.6783\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6694 - loss: 0.5977 - val_accuracy: 0.6783 - val_loss: 0.6192\n",
      "Epoch 49/240\n",
      "\u001b[1m31/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7247 - loss: 0.5941Epoch 49: Loss = 0.6130, Accuracy = 0.6816, Val_Loss = 0.6434, Val_Accuracy = 0.6957\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7200 - loss: 0.5962 - val_accuracy: 0.6957 - val_loss: 0.6434\n",
      "Epoch 50/240\n",
      "\u001b[1m19/34\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7032 - loss: 0.6050 Epoch 50: Loss = 0.6126, Accuracy = 0.6741, Val_Loss = 0.6412, Val_Accuracy = 0.6957\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6904 - loss: 0.6076 - val_accuracy: 0.6957 - val_loss: 0.6412\n",
      "Epoch 51/240\n",
      "\u001b[1m22/34\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6310 - loss: 0.6252Epoch 51: Loss = 0.6122, Accuracy = 0.6667, Val_Loss = 0.6069, Val_Accuracy = 0.6783\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6417 - loss: 0.6201 - val_accuracy: 0.6783 - val_loss: 0.6069\n",
      "Epoch 52/240\n",
      "\u001b[1m30/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7101 - loss: 0.6219Epoch 52: Loss = 0.6125, Accuracy = 0.7020, Val_Loss = 0.6423, Val_Accuracy = 0.6696\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7087 - loss: 0.6210 - val_accuracy: 0.6696 - val_loss: 0.6423\n",
      "Epoch 53/240\n",
      "\u001b[1m27/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7078 - loss: 0.5949Epoch 53: Loss = 0.6129, Accuracy = 0.6797, Val_Loss = 0.6477, Val_Accuracy = 0.6609\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7016 - loss: 0.5975 - val_accuracy: 0.6609 - val_loss: 0.6477\n",
      "Epoch 54/240\n",
      "\u001b[1m29/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6856 - loss: 0.5976Epoch 54: Loss = 0.6169, Accuracy = 0.6704, Val_Loss = 0.6382, Val_Accuracy = 0.6783\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6827 - loss: 0.6004 - val_accuracy: 0.6783 - val_loss: 0.6382\n",
      "Epoch 55/240\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7089 - loss: 0.6159Epoch 55: Loss = 0.6114, Accuracy = 0.6834, Val_Loss = 0.6091, Val_Accuracy = 0.6783\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7082 - loss: 0.6158 - val_accuracy: 0.6783 - val_loss: 0.6091\n",
      "Epoch 56/240\n",
      "\u001b[1m33/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6935 - loss: 0.6213Epoch 56: Loss = 0.6058, Accuracy = 0.6909, Val_Loss = 0.6381, Val_Accuracy = 0.6783\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6934 - loss: 0.6209 - val_accuracy: 0.6783 - val_loss: 0.6381\n",
      "Epoch 57/240\n",
      "\u001b[1m30/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6967 - loss: 0.5914Epoch 57: Loss = 0.6053, Accuracy = 0.6778, Val_Loss = 0.6241, Val_Accuracy = 0.6957\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6939 - loss: 0.5918 - val_accuracy: 0.6957 - val_loss: 0.6241\n",
      "Epoch 58/240\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6833 - loss: 0.6045Epoch 58: Loss = 0.6055, Accuracy = 0.6890, Val_Loss = 0.6216, Val_Accuracy = 0.7043\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6834 - loss: 0.6045 - val_accuracy: 0.7043 - val_loss: 0.6216\n",
      "Epoch 59/240\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6709 - loss: 0.6098Epoch 59: Loss = 0.6054, Accuracy = 0.6834, Val_Loss = 0.6074, Val_Accuracy = 0.6870\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6712 - loss: 0.6097 - val_accuracy: 0.6870 - val_loss: 0.6074\n",
      "Epoch 60/240\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6783 - loss: 0.5985Epoch 60: Loss = 0.6020, Accuracy = 0.6723, Val_Loss = 0.6296, Val_Accuracy = 0.7043\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6782 - loss: 0.5986 - val_accuracy: 0.7043 - val_loss: 0.6296\n",
      "Epoch 61/240\n",
      "\u001b[1m31/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6767 - loss: 0.6023Epoch 61: Loss = 0.6027, Accuracy = 0.7020, Val_Loss = 0.6538, Val_Accuracy = 0.6261\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6792 - loss: 0.6023 - val_accuracy: 0.6261 - val_loss: 0.6538\n",
      "Epoch 62/240\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6279 - loss: 0.6141 Epoch 62: Loss = 0.6106, Accuracy = 0.6592, Val_Loss = 0.6228, Val_Accuracy = 0.6870\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6288 - loss: 0.6140 - val_accuracy: 0.6870 - val_loss: 0.6228\n",
      "Epoch 63/240\n",
      "\u001b[1m30/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6467 - loss: 0.6029Epoch 63: Loss = 0.6098, Accuracy = 0.6425, Val_Loss = 0.6143, Val_Accuracy = 0.6957\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6457 - loss: 0.6031 - val_accuracy: 0.6957 - val_loss: 0.6143\n",
      "Epoch 64/240\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6650 - loss: 0.6119 Epoch 64: Loss = 0.5998, Accuracy = 0.6648, Val_Loss = 0.5946, Val_Accuracy = 0.6870\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6650 - loss: 0.6115 - val_accuracy: 0.6870 - val_loss: 0.5946\n",
      "Epoch 65/240\n",
      "\u001b[1m30/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7215 - loss: 0.5793Epoch 65: Loss = 0.5984, Accuracy = 0.6890, Val_Loss = 0.6972, Val_Accuracy = 0.6087\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7175 - loss: 0.5815 - val_accuracy: 0.6087 - val_loss: 0.6972\n",
      "Epoch 66/240\n",
      "\u001b[1m29/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6218 - loss: 0.6203Epoch 66: Loss = 0.5974, Accuracy = 0.6704, Val_Loss = 0.6178, Val_Accuracy = 0.6609\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6300 - loss: 0.6168 - val_accuracy: 0.6609 - val_loss: 0.6178\n",
      "Epoch 67/240\n",
      "\u001b[1m33/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6813 - loss: 0.5840Epoch 67: Loss = 0.5939, Accuracy = 0.6611, Val_Loss = 0.6129, Val_Accuracy = 0.6696\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6801 - loss: 0.5843 - val_accuracy: 0.6696 - val_loss: 0.6129\n",
      "Epoch 68/240\n",
      "\u001b[1m32/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6722 - loss: 0.6118Epoch 68: Loss = 0.5932, Accuracy = 0.6778, Val_Loss = 0.6166, Val_Accuracy = 0.6522\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6727 - loss: 0.6113 - val_accuracy: 0.6522 - val_loss: 0.6166\n",
      "Epoch 69/240\n",
      "\u001b[1m32/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6748 - loss: 0.5850Epoch 69: Loss = 0.5936, Accuracy = 0.6760, Val_Loss = 0.6549, Val_Accuracy = 0.6087\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6748 - loss: 0.5855 - val_accuracy: 0.6087 - val_loss: 0.6549\n",
      "Epoch 70/240\n",
      "\u001b[1m32/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6539 - loss: 0.5966Epoch 70: Loss = 0.5874, Accuracy = 0.6704, Val_Loss = 0.6144, Val_Accuracy = 0.6435\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6553 - loss: 0.5958 - val_accuracy: 0.6435 - val_loss: 0.6144\n",
      "Epoch 71/240\n",
      "\u001b[1m27/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6892 - loss: 0.5762Epoch 71: Loss = 0.5886, Accuracy = 0.6685, Val_Loss = 0.6212, Val_Accuracy = 0.6522\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6831 - loss: 0.5788 - val_accuracy: 0.6522 - val_loss: 0.6212\n",
      "Epoch 72/240\n",
      "\u001b[1m33/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7036 - loss: 0.5817Epoch 72: Loss = 0.5893, Accuracy = 0.6965, Val_Loss = 0.6356, Val_Accuracy = 0.6261\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7031 - loss: 0.5819 - val_accuracy: 0.6261 - val_loss: 0.6356\n",
      "Epoch 73/240\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6421 - loss: 0.5810Epoch 73: Loss = 0.5871, Accuracy = 0.6574, Val_Loss = 0.6160, Val_Accuracy = 0.6522\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6425 - loss: 0.5812 - val_accuracy: 0.6522 - val_loss: 0.6160\n",
      "Epoch 74/240\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6372 - loss: 0.6175Epoch 74: Loss = 0.5905, Accuracy = 0.6723, Val_Loss = 0.5952, Val_Accuracy = 0.6783\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6382 - loss: 0.6167 - val_accuracy: 0.6783 - val_loss: 0.5952\n",
      "Epoch 75/240\n",
      "\u001b[1m32/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7124 - loss: 0.5535Epoch 75: Loss = 0.5844, Accuracy = 0.6648, Val_Loss = 0.6566, Val_Accuracy = 0.6261\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7083 - loss: 0.5553 - val_accuracy: 0.6261 - val_loss: 0.6566\n",
      "Epoch 76/240\n",
      "\u001b[1m33/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6390 - loss: 0.5756Epoch 76: Loss = 0.5826, Accuracy = 0.6518, Val_Loss = 0.6044, Val_Accuracy = 0.6696\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6397 - loss: 0.5760 - val_accuracy: 0.6696 - val_loss: 0.6044\n",
      "Epoch 77/240\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6860 - loss: 0.5754Epoch 77: Loss = 0.5823, Accuracy = 0.6965, Val_Loss = 0.6424, Val_Accuracy = 0.6348\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6863 - loss: 0.5756 - val_accuracy: 0.6348 - val_loss: 0.6424\n",
      "Epoch 78/240\n",
      "\u001b[1m 3/34\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7292 - loss: 0.5383 Epoch 78: Loss = 0.5903, Accuracy = 0.6350, Val_Loss = 0.6118, Val_Accuracy = 0.6609\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6873 - loss: 0.5590 - val_accuracy: 0.6609 - val_loss: 0.6118\n",
      "Epoch 79/240\n",
      "\u001b[1m 4/34\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6953 - loss: 0.4786 Epoch 79: Loss = 0.5849, Accuracy = 0.6816, Val_Loss = 0.6379, Val_Accuracy = 0.6174\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6953 - loss: 0.5607 - val_accuracy: 0.6174 - val_loss: 0.6379\n",
      "Epoch 80/240\n",
      "\u001b[1m32/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6837 - loss: 0.5756Epoch 80: Loss = 0.5771, Accuracy = 0.6853, Val_Loss = 0.6236, Val_Accuracy = 0.6174\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6839 - loss: 0.5756 - val_accuracy: 0.6174 - val_loss: 0.6236\n",
      "Epoch 81/240\n",
      "\u001b[1m29/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6686 - loss: 0.5909Epoch 81: Loss = 0.5824, Accuracy = 0.6816, Val_Loss = 0.5998, Val_Accuracy = 0.6696\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6709 - loss: 0.5899 - val_accuracy: 0.6696 - val_loss: 0.5998\n",
      "Epoch 82/240\n",
      "\u001b[1m29/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6968 - loss: 0.5658Epoch 82: Loss = 0.5770, Accuracy = 0.6667, Val_Loss = 0.6156, Val_Accuracy = 0.6522\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6929 - loss: 0.5672 - val_accuracy: 0.6522 - val_loss: 0.6156\n",
      "Epoch 83/240\n",
      "\u001b[1m23/34\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6750 - loss: 0.5849Epoch 83: Loss = 0.5763, Accuracy = 0.6853, Val_Loss = 0.6028, Val_Accuracy = 0.6609\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6774 - loss: 0.5821 - val_accuracy: 0.6609 - val_loss: 0.6028\n",
      "Epoch 84/240\n",
      "\u001b[1m26/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6227 - loss: 0.5860Epoch 84: Loss = 0.5779, Accuracy = 0.6667, Val_Loss = 0.6227, Val_Accuracy = 0.6348\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6331 - loss: 0.5835 - val_accuracy: 0.6348 - val_loss: 0.6227\n",
      "Epoch 85/240\n",
      "\u001b[1m23/34\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6021 - loss: 0.6284Epoch 85: Loss = 0.5681, Accuracy = 0.6462, Val_Loss = 0.5824, Val_Accuracy = 0.7217\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6138 - loss: 0.6138 - val_accuracy: 0.7217 - val_loss: 0.5824\n",
      "Epoch 86/240\n",
      "\u001b[1m25/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7479 - loss: 0.5799Epoch 86: Loss = 0.5771, Accuracy = 0.7076, Val_Loss = 0.6218, Val_Accuracy = 0.6435\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7370 - loss: 0.5782 - val_accuracy: 0.6435 - val_loss: 0.6218\n",
      "Epoch 87/240\n",
      "\u001b[1m21/34\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6709 - loss: 0.5600Epoch 87: Loss = 0.5703, Accuracy = 0.6741, Val_Loss = 0.6116, Val_Accuracy = 0.6522\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6751 - loss: 0.5624 - val_accuracy: 0.6522 - val_loss: 0.6116\n",
      "Epoch 88/240\n",
      "\u001b[1m31/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7016 - loss: 0.5503Epoch 88: Loss = 0.5718, Accuracy = 0.6816, Val_Loss = 0.6400, Val_Accuracy = 0.6348\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6995 - loss: 0.5514 - val_accuracy: 0.6348 - val_loss: 0.6400\n",
      "Epoch 89/240\n",
      "\u001b[1m26/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6812 - loss: 0.5373Epoch 89: Loss = 0.5743, Accuracy = 0.6797, Val_Loss = 0.6239, Val_Accuracy = 0.6435\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6829 - loss: 0.5415 - val_accuracy: 0.6435 - val_loss: 0.6239\n",
      "Epoch 90/240\n",
      "\u001b[1m28/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6655 - loss: 0.5702Epoch 90: Loss = 0.5780, Accuracy = 0.6816, Val_Loss = 0.6765, Val_Accuracy = 0.6000\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6682 - loss: 0.5715 - val_accuracy: 0.6000 - val_loss: 0.6765\n",
      "Epoch 91/240\n",
      "\u001b[1m24/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6004 - loss: 0.5943Epoch 91: Loss = 0.5943, Accuracy = 0.6443, Val_Loss = 0.5819, Val_Accuracy = 0.7130\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6100 - loss: 0.5953 - val_accuracy: 0.7130 - val_loss: 0.5819\n",
      "Epoch 92/240\n",
      "\u001b[1m26/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7121 - loss: 0.5738Epoch 92: Loss = 0.5767, Accuracy = 0.6816, Val_Loss = 0.6508, Val_Accuracy = 0.6000\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7050 - loss: 0.5741 - val_accuracy: 0.6000 - val_loss: 0.6508\n",
      "Epoch 93/240\n",
      "\u001b[1m23/34\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5856 - loss: 0.5849Epoch 93: Loss = 0.5802, Accuracy = 0.6704, Val_Loss = 0.5969, Val_Accuracy = 0.6609\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6142 - loss: 0.5838 - val_accuracy: 0.6609 - val_loss: 0.5969\n",
      "Epoch 94/240\n",
      "\u001b[1m25/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6648 - loss: 0.5869Epoch 94: Loss = 0.5692, Accuracy = 0.6909, Val_Loss = 0.6086, Val_Accuracy = 0.6522\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6694 - loss: 0.5828 - val_accuracy: 0.6522 - val_loss: 0.6086\n",
      "Epoch 95/240\n",
      "\u001b[1m23/34\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6415 - loss: 0.5809Epoch 95: Loss = 0.5620, Accuracy = 0.6704, Val_Loss = 0.5940, Val_Accuracy = 0.6957\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6475 - loss: 0.5782 - val_accuracy: 0.6957 - val_loss: 0.5940\n",
      "Epoch 96/240\n",
      "\u001b[1m23/34\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7076 - loss: 0.5387Epoch 96: Loss = 0.5735, Accuracy = 0.6872, Val_Loss = 0.6222, Val_Accuracy = 0.6435\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7006 - loss: 0.5505 - val_accuracy: 0.6435 - val_loss: 0.6222\n",
      "Epoch 97/240\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6486 - loss: 0.5875Epoch 97: Loss = 0.5648, Accuracy = 0.6872, Val_Loss = 0.5743, Val_Accuracy = 0.7043\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6497 - loss: 0.5868 - val_accuracy: 0.7043 - val_loss: 0.5743\n",
      "Epoch 98/240\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6748 - loss: 0.5721Epoch 98: Loss = 0.5697, Accuracy = 0.6648, Val_Loss = 0.5978, Val_Accuracy = 0.6870\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6745 - loss: 0.5720 - val_accuracy: 0.6870 - val_loss: 0.5978\n",
      "Epoch 99/240\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6713 - loss: 0.5875Epoch 99: Loss = 0.5647, Accuracy = 0.6927, Val_Loss = 0.6003, Val_Accuracy = 0.6696\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6719 - loss: 0.5869 - val_accuracy: 0.6696 - val_loss: 0.6003\n",
      "Epoch 100/240\n",
      "\u001b[1m25/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6970 - loss: 0.5674Epoch 100: Loss = 0.5632, Accuracy = 0.6909, Val_Loss = 0.6830, Val_Accuracy = 0.6000\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6980 - loss: 0.5656 - val_accuracy: 0.6000 - val_loss: 0.6830\n",
      "Epoch 101/240\n",
      "\u001b[1m32/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6523 - loss: 0.5602Epoch 101: Loss = 0.5696, Accuracy = 0.6648, Val_Loss = 0.6141, Val_Accuracy = 0.6609\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6534 - loss: 0.5608 - val_accuracy: 0.6609 - val_loss: 0.6141\n",
      "Epoch 102/240\n",
      "\u001b[1m30/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6737 - loss: 0.5511Epoch 102: Loss = 0.5632, Accuracy = 0.6834, Val_Loss = 0.6199, Val_Accuracy = 0.6435\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6750 - loss: 0.5525 - val_accuracy: 0.6435 - val_loss: 0.6199\n",
      "Epoch 103/240\n",
      "\u001b[1m31/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7219 - loss: 0.5438Epoch 103: Loss = 0.5617, Accuracy = 0.6853, Val_Loss = 0.6367, Val_Accuracy = 0.6261\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7175 - loss: 0.5461 - val_accuracy: 0.6261 - val_loss: 0.6367\n",
      "Epoch 104/240\n",
      "\u001b[1m 2/34\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5938 - loss: 0.6424 Epoch 104: Loss = 0.5655, Accuracy = 0.7076, Val_Loss = 0.5828, Val_Accuracy = 0.7130\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6537 - loss: 0.5924 - val_accuracy: 0.7130 - val_loss: 0.5828\n",
      "Epoch 105/240\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6825 - loss: 0.5753Epoch 105: Loss = 0.5567, Accuracy = 0.7002, Val_Loss = 0.5812, Val_Accuracy = 0.6957\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6830 - loss: 0.5748 - val_accuracy: 0.6957 - val_loss: 0.5812\n",
      "Epoch 106/240\n",
      "\u001b[1m31/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6844 - loss: 0.5821Epoch 106: Loss = 0.5662, Accuracy = 0.6760, Val_Loss = 0.6017, Val_Accuracy = 0.6870\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6834 - loss: 0.5816 - val_accuracy: 0.6870 - val_loss: 0.6017\n",
      "Epoch 107/240\n",
      "\u001b[1m28/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6686 - loss: 0.5764Epoch 107: Loss = 0.5514, Accuracy = 0.7002, Val_Loss = 0.6357, Val_Accuracy = 0.6261\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6742 - loss: 0.5719 - val_accuracy: 0.6261 - val_loss: 0.6357\n",
      "Epoch 108/240\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6982 - loss: 0.5486Epoch 108: Loss = 0.5530, Accuracy = 0.6927, Val_Loss = 0.6309, Val_Accuracy = 0.6261\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6981 - loss: 0.5487 - val_accuracy: 0.6261 - val_loss: 0.6309\n",
      "Epoch 109/240\n",
      "\u001b[1m22/34\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7249 - loss: 0.5050Epoch 109: Loss = 0.5559, Accuracy = 0.6834, Val_Loss = 0.6064, Val_Accuracy = 0.6609\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7108 - loss: 0.5242 - val_accuracy: 0.6609 - val_loss: 0.6064\n",
      "Epoch 110/240\n",
      "\u001b[1m24/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6729 - loss: 0.5297Epoch 110: Loss = 0.5582, Accuracy = 0.6778, Val_Loss = 0.6720, Val_Accuracy = 0.6000\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6775 - loss: 0.5360 - val_accuracy: 0.6000 - val_loss: 0.6720\n",
      "Epoch 111/240\n",
      "\u001b[1m29/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6880 - loss: 0.5579Epoch 111: Loss = 0.5602, Accuracy = 0.6834, Val_Loss = 0.6227, Val_Accuracy = 0.6261\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6875 - loss: 0.5581 - val_accuracy: 0.6261 - val_loss: 0.6227\n",
      "Epoch 112/240\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7150 - loss: 0.5386Epoch 112: Loss = 0.5576, Accuracy = 0.6760, Val_Loss = 0.6023, Val_Accuracy = 0.6609\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7129 - loss: 0.5391 - val_accuracy: 0.6609 - val_loss: 0.6023\n",
      "Epoch 113/240\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6998 - loss: 0.5437Epoch 113: Loss = 0.5515, Accuracy = 0.6965, Val_Loss = 0.5968, Val_Accuracy = 0.6957\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6997 - loss: 0.5439 - val_accuracy: 0.6957 - val_loss: 0.5968\n",
      "Epoch 114/240\n",
      "\u001b[1m 2/34\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7500 - loss: 0.5712 Epoch 114: Loss = 0.5531, Accuracy = 0.6965, Val_Loss = 0.6193, Val_Accuracy = 0.6609\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7178 - loss: 0.5422 - val_accuracy: 0.6609 - val_loss: 0.6193\n",
      "Epoch 115/240\n",
      "\u001b[1m 7/34\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7891 - loss: 0.5042 Epoch 115: Loss = 0.5628, Accuracy = 0.6890, Val_Loss = 0.5889, Val_Accuracy = 0.7130\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7166 - loss: 0.5370 - val_accuracy: 0.7130 - val_loss: 0.5889\n",
      "Epoch 116/240\n",
      "\u001b[1m 3/34\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7431 - loss: 0.4980 Epoch 116: Loss = 0.5489, Accuracy = 0.6853, Val_Loss = 0.6006, Val_Accuracy = 0.7217\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6924 - loss: 0.5460 - val_accuracy: 0.7217 - val_loss: 0.6006\n",
      "Epoch 117/240\n",
      "\u001b[1m28/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6785 - loss: 0.5885Epoch 117: Loss = 0.5686, Accuracy = 0.6648, Val_Loss = 0.5582, Val_Accuracy = 0.7304\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6760 - loss: 0.5862 - val_accuracy: 0.7304 - val_loss: 0.5582\n",
      "Epoch 118/240\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7100 - loss: 0.5800 Epoch 118: Loss = 0.5606, Accuracy = 0.7020, Val_Loss = 0.6104, Val_Accuracy = 0.6870\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7098 - loss: 0.5795 - val_accuracy: 0.6870 - val_loss: 0.6104\n",
      "Epoch 119/240\n",
      "\u001b[1m 3/34\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6875 - loss: 0.4998Epoch 119: Loss = 0.5445, Accuracy = 0.7020, Val_Loss = 0.6332, Val_Accuracy = 0.6348\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7095 - loss: 0.5417 - val_accuracy: 0.6348 - val_loss: 0.6332\n",
      "Epoch 120/240\n",
      "\u001b[1m33/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6746 - loss: 0.5593Epoch 120: Loss = 0.5482, Accuracy = 0.6872, Val_Loss = 0.5983, Val_Accuracy = 0.7130\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6753 - loss: 0.5587 - val_accuracy: 0.7130 - val_loss: 0.5983\n",
      "Epoch 121/240\n",
      "\u001b[1m 6/34\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7038 - loss: 0.5843 Epoch 121: Loss = 0.5600, Accuracy = 0.6965, Val_Loss = 0.5994, Val_Accuracy = 0.7043\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6966 - loss: 0.5610 - val_accuracy: 0.7043 - val_loss: 0.5994\n",
      "Epoch 122/240\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6985 - loss: 0.5238Epoch 122: Loss = 0.5521, Accuracy = 0.7039, Val_Loss = 0.6843, Val_Accuracy = 0.5739\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6986 - loss: 0.5246 - val_accuracy: 0.5739 - val_loss: 0.6843\n",
      "Epoch 123/240\n",
      "\u001b[1m20/34\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6096 - loss: 0.5809 Epoch 123: Loss = 0.5566, Accuracy = 0.6853, Val_Loss = 0.6096, Val_Accuracy = 0.6783\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6404 - loss: 0.5715 - val_accuracy: 0.6783 - val_loss: 0.6096\n",
      "Epoch 124/240\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6813 - loss: 0.5616Epoch 124: Loss = 0.5495, Accuracy = 0.7002, Val_Loss = 0.6878, Val_Accuracy = 0.5913\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6818 - loss: 0.5613 - val_accuracy: 0.5913 - val_loss: 0.6878\n",
      "Epoch 125/240\n",
      "\u001b[1m26/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6667 - loss: 0.5353Epoch 125: Loss = 0.5697, Accuracy = 0.6927, Val_Loss = 0.6187, Val_Accuracy = 0.6696\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6720 - loss: 0.5439 - val_accuracy: 0.6696 - val_loss: 0.6187\n",
      "Epoch 126/240\n",
      "\u001b[1m30/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6694 - loss: 0.5492Epoch 126: Loss = 0.5479, Accuracy = 0.6778, Val_Loss = 0.6525, Val_Accuracy = 0.6261\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6718 - loss: 0.5491 - val_accuracy: 0.6261 - val_loss: 0.6525\n",
      "Epoch 127/240\n",
      "\u001b[1m30/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6789 - loss: 0.5361Epoch 127: Loss = 0.5475, Accuracy = 0.6909, Val_Loss = 0.6324, Val_Accuracy = 0.6348\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6812 - loss: 0.5376 - val_accuracy: 0.6348 - val_loss: 0.6324\n",
      "Epoch 128/240\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6888 - loss: 0.5667Epoch 128: Loss = 0.5480, Accuracy = 0.6909, Val_Loss = 0.6069, Val_Accuracy = 0.6957\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6889 - loss: 0.5662 - val_accuracy: 0.6957 - val_loss: 0.6069\n",
      "Epoch 129/240\n",
      "\u001b[1m 7/34\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6684 - loss: 0.5410Epoch 129: Loss = 0.5451, Accuracy = 0.6834, Val_Loss = 0.6149, Val_Accuracy = 0.6870\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6563 - loss: 0.5456 - val_accuracy: 0.6870 - val_loss: 0.6149\n",
      "Epoch 130/240\n",
      "\u001b[1m33/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6811 - loss: 0.5428Epoch 130: Loss = 0.5380, Accuracy = 0.6797, Val_Loss = 0.6252, Val_Accuracy = 0.6957\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6811 - loss: 0.5426 - val_accuracy: 0.6957 - val_loss: 0.6252\n",
      "Epoch 131/240\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6985 - loss: 0.5296Epoch 131: Loss = 0.5426, Accuracy = 0.7020, Val_Loss = 0.6338, Val_Accuracy = 0.6435\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6986 - loss: 0.5300 - val_accuracy: 0.6435 - val_loss: 0.6338\n",
      "Epoch 132/240\n",
      "\u001b[1m27/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6868 - loss: 0.5545Epoch 132: Loss = 0.5450, Accuracy = 0.6983, Val_Loss = 0.6656, Val_Accuracy = 0.6174\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6891 - loss: 0.5530 - val_accuracy: 0.6174 - val_loss: 0.6656\n",
      "Epoch 133/240\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6977 - loss: 0.5310Epoch 133: Loss = 0.5384, Accuracy = 0.7114, Val_Loss = 0.6427, Val_Accuracy = 0.6348\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6981 - loss: 0.5312 - val_accuracy: 0.6348 - val_loss: 0.6427\n",
      "Epoch 134/240\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6346 - loss: 0.5621Epoch 134: Loss = 0.5357, Accuracy = 0.6797, Val_Loss = 0.6068, Val_Accuracy = 0.7043\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6359 - loss: 0.5614 - val_accuracy: 0.7043 - val_loss: 0.6068\n",
      "Epoch 135/240\n",
      "\u001b[1m23/34\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6974 - loss: 0.5286Epoch 135: Loss = 0.5351, Accuracy = 0.6946, Val_Loss = 0.6822, Val_Accuracy = 0.5652\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6974 - loss: 0.5290 - val_accuracy: 0.5652 - val_loss: 0.6822\n",
      "Epoch 136/240\n",
      "\u001b[1m32/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6123 - loss: 0.5632Epoch 136: Loss = 0.5518, Accuracy = 0.6760, Val_Loss = 0.6576, Val_Accuracy = 0.6174\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6171 - loss: 0.5629 - val_accuracy: 0.6174 - val_loss: 0.6576\n",
      "Epoch 137/240\n",
      "\u001b[1m28/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6584 - loss: 0.5300Epoch 137: Loss = 0.5360, Accuracy = 0.6704, Val_Loss = 0.5995, Val_Accuracy = 0.7043\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6606 - loss: 0.5317 - val_accuracy: 0.7043 - val_loss: 0.5995\n",
      "Epoch 138/240\n",
      "\u001b[1m24/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6888 - loss: 0.5649Epoch 138: Loss = 0.5355, Accuracy = 0.7188, Val_Loss = 0.6283, Val_Accuracy = 0.6783\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7020 - loss: 0.5591 - val_accuracy: 0.6783 - val_loss: 0.6283\n",
      "Epoch 139/240\n",
      "\u001b[1m32/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6609 - loss: 0.5317Epoch 139: Loss = 0.5341, Accuracy = 0.6797, Val_Loss = 0.6156, Val_Accuracy = 0.6696\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6624 - loss: 0.5318 - val_accuracy: 0.6696 - val_loss: 0.6156\n",
      "Epoch 140/240\n",
      "\u001b[1m31/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7066 - loss: 0.5559Epoch 140: Loss = 0.5357, Accuracy = 0.7114, Val_Loss = 0.6286, Val_Accuracy = 0.6609\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7071 - loss: 0.5547 - val_accuracy: 0.6609 - val_loss: 0.6286\n",
      "Epoch 141/240\n",
      "\u001b[1m26/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6631 - loss: 0.5381  Epoch 141: Loss = 0.5337, Accuracy = 0.6853, Val_Loss = 0.6082, Val_Accuracy = 0.7130\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6663 - loss: 0.5388 - val_accuracy: 0.7130 - val_loss: 0.6082\n",
      "Epoch 142/240\n",
      "\u001b[1m26/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7205 - loss: 0.5203Epoch 142: Loss = 0.5324, Accuracy = 0.7039, Val_Loss = 0.6323, Val_Accuracy = 0.6696\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7176 - loss: 0.5222 - val_accuracy: 0.6696 - val_loss: 0.6323\n",
      "Epoch 143/240\n",
      "\u001b[1m33/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6574 - loss: 0.5499Epoch 143: Loss = 0.5384, Accuracy = 0.6983, Val_Loss = 0.6231, Val_Accuracy = 0.6783\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6596 - loss: 0.5492 - val_accuracy: 0.6783 - val_loss: 0.6231\n",
      "Epoch 144/240\n",
      "\u001b[1m29/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6608 - loss: 0.5571Epoch 144: Loss = 0.5346, Accuracy = 0.6965, Val_Loss = 0.6286, Val_Accuracy = 0.6696\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6669 - loss: 0.5553 - val_accuracy: 0.6696 - val_loss: 0.6286\n",
      "Epoch 145/240\n",
      "\u001b[1m27/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6590 - loss: 0.5399Epoch 145: Loss = 0.5316, Accuracy = 0.6797, Val_Loss = 0.6118, Val_Accuracy = 0.6957\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6643 - loss: 0.5379 - val_accuracy: 0.6957 - val_loss: 0.6118\n",
      "Epoch 146/240\n",
      "\u001b[1m29/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6924 - loss: 0.5342Epoch 146: Loss = 0.5282, Accuracy = 0.7207, Val_Loss = 0.6471, Val_Accuracy = 0.6435\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6968 - loss: 0.5337 - val_accuracy: 0.6435 - val_loss: 0.6471\n",
      "Epoch 147/240\n",
      "\u001b[1m30/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6975 - loss: 0.5291Epoch 147: Loss = 0.5304, Accuracy = 0.6983, Val_Loss = 0.6354, Val_Accuracy = 0.6522\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6985 - loss: 0.5287 - val_accuracy: 0.6522 - val_loss: 0.6354\n",
      "Epoch 148/240\n",
      "\u001b[1m31/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7058 - loss: 0.5143Epoch 148: Loss = 0.5389, Accuracy = 0.7020, Val_Loss = 0.6130, Val_Accuracy = 0.7217\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7055 - loss: 0.5149 - val_accuracy: 0.7217 - val_loss: 0.6130\n",
      "Epoch 149/240\n",
      "\u001b[1m22/34\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7092 - loss: 0.5314Epoch 149: Loss = 0.5316, Accuracy = 0.7058, Val_Loss = 0.6365, Val_Accuracy = 0.6348\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7072 - loss: 0.5318 - val_accuracy: 0.6348 - val_loss: 0.6365\n",
      "Epoch 150/240\n",
      "\u001b[1m25/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7008 - loss: 0.5025Epoch 150: Loss = 0.5325, Accuracy = 0.7020, Val_Loss = 0.6497, Val_Accuracy = 0.6087\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7013 - loss: 0.5116 - val_accuracy: 0.6087 - val_loss: 0.6497\n",
      "Epoch 151/240\n",
      "\u001b[1m26/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6750 - loss: 0.5240Epoch 151: Loss = 0.5286, Accuracy = 0.7020, Val_Loss = 0.6133, Val_Accuracy = 0.7217\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6834 - loss: 0.5246 - val_accuracy: 0.7217 - val_loss: 0.6133\n",
      "Epoch 152/240\n",
      "\u001b[1m28/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7056 - loss: 0.5299Epoch 152: Loss = 0.5252, Accuracy = 0.7095, Val_Loss = 0.6407, Val_Accuracy = 0.6522\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7065 - loss: 0.5296 - val_accuracy: 0.6522 - val_loss: 0.6407\n",
      "Epoch 153/240\n",
      "\u001b[1m27/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6745 - loss: 0.5222Epoch 153: Loss = 0.5264, Accuracy = 0.6946, Val_Loss = 0.5980, Val_Accuracy = 0.7304\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6793 - loss: 0.5233 - val_accuracy: 0.7304 - val_loss: 0.5980\n",
      "Epoch 154/240\n",
      "\u001b[1m25/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7076 - loss: 0.5367Epoch 154: Loss = 0.5307, Accuracy = 0.7039, Val_Loss = 0.5972, Val_Accuracy = 0.7217\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7042 - loss: 0.5352 - val_accuracy: 0.7217 - val_loss: 0.5972\n",
      "Epoch 155/240\n",
      "\u001b[1m23/34\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6513 - loss: 0.5961 Epoch 155: Loss = 0.5254, Accuracy = 0.6965, Val_Loss = 0.6094, Val_Accuracy = 0.7304\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6622 - loss: 0.5747 - val_accuracy: 0.7304 - val_loss: 0.6094\n",
      "Epoch 156/240\n",
      "\u001b[1m24/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6647 - loss: 0.5654Epoch 156: Loss = 0.5464, Accuracy = 0.6983, Val_Loss = 0.5743, Val_Accuracy = 0.7217\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6743 - loss: 0.5594 - val_accuracy: 0.7217 - val_loss: 0.5743\n",
      "Epoch 157/240\n",
      "\u001b[1m24/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7100 - loss: 0.5525Epoch 157: Loss = 0.5349, Accuracy = 0.6853, Val_Loss = 0.6225, Val_Accuracy = 0.6870\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7054 - loss: 0.5491 - val_accuracy: 0.6870 - val_loss: 0.6225\n",
      "Epoch 158/240\n",
      "\u001b[1m24/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6444 - loss: 0.5601  Epoch 158: Loss = 0.5285, Accuracy = 0.7151, Val_Loss = 0.6414, Val_Accuracy = 0.6957\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6644 - loss: 0.5509 - val_accuracy: 0.6957 - val_loss: 0.6414\n",
      "Epoch 159/240\n",
      "\u001b[1m27/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7004 - loss: 0.5308Epoch 159: Loss = 0.5303, Accuracy = 0.6909, Val_Loss = 0.6151, Val_Accuracy = 0.7391\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6980 - loss: 0.5302 - val_accuracy: 0.7391 - val_loss: 0.6151\n",
      "Epoch 160/240\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6992 - loss: 0.5234Epoch 160: Loss = 0.5263, Accuracy = 0.7020, Val_Loss = 0.5960, Val_Accuracy = 0.7130\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6993 - loss: 0.5234 - val_accuracy: 0.7130 - val_loss: 0.5960\n",
      "Epoch 161/240\n",
      "\u001b[1m33/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7271 - loss: 0.5196Epoch 161: Loss = 0.5343, Accuracy = 0.7095, Val_Loss = 0.5809, Val_Accuracy = 0.7217\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7261 - loss: 0.5200 - val_accuracy: 0.7217 - val_loss: 0.5809\n",
      "Epoch 162/240\n",
      "\u001b[1m31/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6893 - loss: 0.5431Epoch 162: Loss = 0.5321, Accuracy = 0.7095, Val_Loss = 0.6371, Val_Accuracy = 0.7130\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6914 - loss: 0.5422 - val_accuracy: 0.7130 - val_loss: 0.6371\n",
      "Epoch 163/240\n",
      "\u001b[1m 4/34\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6745 - loss: 0.4915Epoch 163: Loss = 0.5208, Accuracy = 0.6909, Val_Loss = 0.6062, Val_Accuracy = 0.7130\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6912 - loss: 0.5075 - val_accuracy: 0.7130 - val_loss: 0.6062\n",
      "Epoch 164/240\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6735 - loss: 0.5315 Epoch 164: Loss = 0.5194, Accuracy = 0.6797, Val_Loss = 0.5977, Val_Accuracy = 0.7304\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6737 - loss: 0.5312 - val_accuracy: 0.7304 - val_loss: 0.5977\n",
      "Epoch 165/240\n",
      "\u001b[1m32/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7673 - loss: 0.5053Epoch 165: Loss = 0.5233, Accuracy = 0.7412, Val_Loss = 0.6304, Val_Accuracy = 0.6609\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7651 - loss: 0.5069 - val_accuracy: 0.6609 - val_loss: 0.6304\n",
      "Epoch 166/240\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6660 - loss: 0.5405Epoch 166: Loss = 0.5369, Accuracy = 0.6741, Val_Loss = 0.5862, Val_Accuracy = 0.7304\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6662 - loss: 0.5404 - val_accuracy: 0.7304 - val_loss: 0.5862\n",
      "Epoch 167/240\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7164 - loss: 0.5360Epoch 167: Loss = 0.5305, Accuracy = 0.7225, Val_Loss = 0.6369, Val_Accuracy = 0.6696\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7166 - loss: 0.5358 - val_accuracy: 0.6696 - val_loss: 0.6369\n",
      "Epoch 168/240\n",
      "\u001b[1m33/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6809 - loss: 0.5226Epoch 168: Loss = 0.5142, Accuracy = 0.6890, Val_Loss = 0.6208, Val_Accuracy = 0.7304\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6813 - loss: 0.5223 - val_accuracy: 0.7304 - val_loss: 0.6208\n",
      "Epoch 169/240\n",
      "\u001b[1m 6/34\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7060 - loss: 0.5917 Epoch 169: Loss = 0.5334, Accuracy = 0.6983, Val_Loss = 0.6459, Val_Accuracy = 0.6348\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7128 - loss: 0.5542 - val_accuracy: 0.6348 - val_loss: 0.6459\n",
      "Epoch 170/240\n",
      "\u001b[1m31/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7202 - loss: 0.5038Epoch 170: Loss = 0.5286, Accuracy = 0.7020, Val_Loss = 0.6566, Val_Accuracy = 0.6087\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7185 - loss: 0.5052 - val_accuracy: 0.6087 - val_loss: 0.6566\n",
      "Epoch 171/240\n",
      "\u001b[1m 2/34\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6875 - loss: 0.5524 Epoch 171: Loss = 0.5148, Accuracy = 0.7039, Val_Loss = 0.6473, Val_Accuracy = 0.6261\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6925 - loss: 0.5336 - val_accuracy: 0.6261 - val_loss: 0.6473\n",
      "Epoch 172/240\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7328 - loss: 0.5011Epoch 172: Loss = 0.5170, Accuracy = 0.7039, Val_Loss = 0.6169, Val_Accuracy = 0.7130\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7320 - loss: 0.5015 - val_accuracy: 0.7130 - val_loss: 0.6169\n",
      "Epoch 173/240\n",
      "\u001b[1m26/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7121 - loss: 0.5074Epoch 173: Loss = 0.5181, Accuracy = 0.7281, Val_Loss = 0.6174, Val_Accuracy = 0.7304\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7156 - loss: 0.5094 - val_accuracy: 0.7304 - val_loss: 0.6174\n",
      "Epoch 174/240\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7377 - loss: 0.4972Epoch 174: Loss = 0.5270, Accuracy = 0.6965, Val_Loss = 0.6032, Val_Accuracy = 0.7217\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7365 - loss: 0.4980 - val_accuracy: 0.7217 - val_loss: 0.6032\n",
      "Epoch 175/240\n",
      "\u001b[1m30/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7577 - loss: 0.5110Epoch 175: Loss = 0.5232, Accuracy = 0.7244, Val_Loss = 0.6054, Val_Accuracy = 0.7304\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7529 - loss: 0.5131 - val_accuracy: 0.7304 - val_loss: 0.6054\n",
      "Epoch 176/240\n",
      "\u001b[1m27/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7644 - loss: 0.5185Epoch 176: Loss = 0.5386, Accuracy = 0.7207, Val_Loss = 0.5998, Val_Accuracy = 0.7217\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7538 - loss: 0.5231 - val_accuracy: 0.7217 - val_loss: 0.5998\n",
      "Epoch 177/240\n",
      "\u001b[1m24/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6922 - loss: 0.5327  Epoch 177: Loss = 0.5168, Accuracy = 0.7095, Val_Loss = 0.6109, Val_Accuracy = 0.7304\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6963 - loss: 0.5288 - val_accuracy: 0.7304 - val_loss: 0.6109\n",
      "Epoch 178/240\n",
      "\u001b[1m33/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7345 - loss: 0.4995Epoch 178: Loss = 0.5211, Accuracy = 0.7114, Val_Loss = 0.6201, Val_Accuracy = 0.6870\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7332 - loss: 0.5006 - val_accuracy: 0.6870 - val_loss: 0.6201\n",
      "Epoch 179/240\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7021 - loss: 0.5151Epoch 179: Loss = 0.5180, Accuracy = 0.6983, Val_Loss = 0.6372, Val_Accuracy = 0.6435\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7020 - loss: 0.5152 - val_accuracy: 0.6435 - val_loss: 0.6372\n",
      "Epoch 180/240\n",
      "\u001b[1m31/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6965 - loss: 0.5092Epoch 180: Loss = 0.5171, Accuracy = 0.7020, Val_Loss = 0.6925, Val_Accuracy = 0.5913\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6969 - loss: 0.5100 - val_accuracy: 0.5913 - val_loss: 0.6925\n",
      "Epoch 181/240\n",
      "\u001b[1m27/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6614 - loss: 0.5198Epoch 181: Loss = 0.5252, Accuracy = 0.6965, Val_Loss = 0.6199, Val_Accuracy = 0.6696\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6688 - loss: 0.5213 - val_accuracy: 0.6696 - val_loss: 0.6199\n",
      "Epoch 182/240\n",
      "\u001b[1m 1/34\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.6250 - loss: 0.5346: 0.5112Epoch 182: Loss = 0.5188, Accuracy = 0.6872, Val_Loss = 0.6103, Val_Accuracy = 0.7304\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7131 - loss: 0.5066 - val_accuracy: 0.7304 - val_loss: 0.6103\n",
      "Epoch 183/240\n",
      "\u001b[1m30/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7360 - loss: 0.5015Epoch 183: Loss = 0.5200, Accuracy = 0.7430, Val_Loss = 0.6683, Val_Accuracy = 0.5826\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7367 - loss: 0.5026 - val_accuracy: 0.5826 - val_loss: 0.6683\n",
      "Epoch 184/240\n",
      "\u001b[1m29/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6436 - loss: 0.5563Epoch 184: Loss = 0.5401, Accuracy = 0.7058, Val_Loss = 0.5808, Val_Accuracy = 0.7217\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6542 - loss: 0.5544 - val_accuracy: 0.7217 - val_loss: 0.5808\n",
      "Epoch 185/240\n",
      "\u001b[1m31/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7034 - loss: 0.5212Epoch 185: Loss = 0.5154, Accuracy = 0.6946, Val_Loss = 0.6019, Val_Accuracy = 0.7304\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7023 - loss: 0.5209 - val_accuracy: 0.7304 - val_loss: 0.6019\n",
      "Epoch 186/240\n",
      "\u001b[1m31/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6914 - loss: 0.5347Epoch 186: Loss = 0.5214, Accuracy = 0.6890, Val_Loss = 0.6356, Val_Accuracy = 0.6522\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6912 - loss: 0.5343 - val_accuracy: 0.6522 - val_loss: 0.6356\n",
      "Epoch 187/240\n",
      "\u001b[1m32/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7960 - loss: 0.4824Epoch 187: Loss = 0.5177, Accuracy = 0.7337, Val_Loss = 0.6246, Val_Accuracy = 0.6696\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7907 - loss: 0.4854 - val_accuracy: 0.6696 - val_loss: 0.6246\n",
      "Epoch 188/240\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6954 - loss: 0.5350Epoch 188: Loss = 0.5211, Accuracy = 0.6909, Val_Loss = 0.6465, Val_Accuracy = 0.7130\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6952 - loss: 0.5346 - val_accuracy: 0.7130 - val_loss: 0.6465\n",
      "Epoch 189/240\n",
      "\u001b[1m18/34\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7233 - loss: 0.5161 Epoch 189: Loss = 0.5194, Accuracy = 0.7188, Val_Loss = 0.6227, Val_Accuracy = 0.6522\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7148 - loss: 0.5163 - val_accuracy: 0.6522 - val_loss: 0.6227\n",
      "Epoch 190/240\n",
      "\u001b[1m23/34\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6332 - loss: 0.5383 Epoch 190: Loss = 0.5272, Accuracy = 0.6946, Val_Loss = 0.6671, Val_Accuracy = 0.6087\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6575 - loss: 0.5327 - val_accuracy: 0.6087 - val_loss: 0.6671\n",
      "Epoch 191/240\n",
      "\u001b[1m28/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6898 - loss: 0.5415Epoch 191: Loss = 0.5256, Accuracy = 0.7095, Val_Loss = 0.6072, Val_Accuracy = 0.7304\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6940 - loss: 0.5393 - val_accuracy: 0.7304 - val_loss: 0.6072\n",
      "Epoch 192/240\n",
      "\u001b[1m20/34\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7098 - loss: 0.4966 Epoch 192: Loss = 0.5325, Accuracy = 0.6834, Val_Loss = 0.6502, Val_Accuracy = 0.6000\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7091 - loss: 0.5080 - val_accuracy: 0.6000 - val_loss: 0.6502\n",
      "Epoch 193/240\n",
      "\u001b[1m22/34\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7368 - loss: 0.5141 Epoch 193: Loss = 0.5154, Accuracy = 0.7374, Val_Loss = 0.6868, Val_Accuracy = 0.6174\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7409 - loss: 0.5130 - val_accuracy: 0.6174 - val_loss: 0.6868\n",
      "Epoch 194/240\n",
      "\u001b[1m26/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6810 - loss: 0.4985Epoch 194: Loss = 0.5313, Accuracy = 0.6667, Val_Loss = 0.6420, Val_Accuracy = 0.6174\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6786 - loss: 0.5022 - val_accuracy: 0.6174 - val_loss: 0.6420\n",
      "Epoch 195/240\n",
      "\u001b[1m28/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7051 - loss: 0.5093Epoch 195: Loss = 0.5102, Accuracy = 0.7132, Val_Loss = 0.6386, Val_Accuracy = 0.6522\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7065 - loss: 0.5092 - val_accuracy: 0.6522 - val_loss: 0.6386\n",
      "Epoch 196/240\n",
      "\u001b[1m16/34\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6744 - loss: 0.5251Epoch 196: Loss = 0.5318, Accuracy = 0.6816, Val_Loss = 0.5943, Val_Accuracy = 0.7217\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6802 - loss: 0.5246 - val_accuracy: 0.7217 - val_loss: 0.5943\n",
      "Epoch 197/240\n",
      "\u001b[1m18/34\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6986 - loss: 0.5719Epoch 197: Loss = 0.5156, Accuracy = 0.7244, Val_Loss = 0.6335, Val_Accuracy = 0.7217\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7113 - loss: 0.5528 - val_accuracy: 0.7217 - val_loss: 0.6335\n",
      "Epoch 198/240\n",
      "\u001b[1m33/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6936 - loss: 0.5217Epoch 198: Loss = 0.5235, Accuracy = 0.6760, Val_Loss = 0.5858, Val_Accuracy = 0.7304\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6926 - loss: 0.5217 - val_accuracy: 0.7304 - val_loss: 0.5858\n",
      "Epoch 199/240\n",
      "\u001b[1m 5/34\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7667 - loss: 0.5604Epoch 199: Loss = 0.5216, Accuracy = 0.7393, Val_Loss = 0.6267, Val_Accuracy = 0.6783\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7416 - loss: 0.5278 - val_accuracy: 0.6783 - val_loss: 0.6267\n",
      "Epoch 200/240\n",
      "\u001b[1m33/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7136 - loss: 0.5313Epoch 200: Loss = 0.5106, Accuracy = 0.7151, Val_Loss = 0.6250, Val_Accuracy = 0.6696\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7137 - loss: 0.5309 - val_accuracy: 0.6696 - val_loss: 0.6250\n",
      "Epoch 201/240\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7126 - loss: 0.4955Epoch 201: Loss = 0.5111, Accuracy = 0.7151, Val_Loss = 0.6117, Val_Accuracy = 0.7217\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7126 - loss: 0.4960 - val_accuracy: 0.7217 - val_loss: 0.6117\n",
      "Epoch 202/240\n",
      "\u001b[1m28/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7351 - loss: 0.5114Epoch 202: Loss = 0.5091, Accuracy = 0.7281, Val_Loss = 0.6381, Val_Accuracy = 0.6522\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7333 - loss: 0.5115 - val_accuracy: 0.6522 - val_loss: 0.6381\n",
      "Epoch 203/240\n",
      "\u001b[1m30/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7271 - loss: 0.4812Epoch 203: Loss = 0.5090, Accuracy = 0.6927, Val_Loss = 0.6086, Val_Accuracy = 0.7217\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7222 - loss: 0.4853 - val_accuracy: 0.7217 - val_loss: 0.6086\n",
      "Epoch 204/240\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7120 - loss: 0.5188Epoch 204: Loss = 0.5273, Accuracy = 0.7132, Val_Loss = 0.6694, Val_Accuracy = 0.6000\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7120 - loss: 0.5190 - val_accuracy: 0.6000 - val_loss: 0.6694\n",
      "Epoch 205/240\n",
      "\u001b[1m22/34\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6459 - loss: 0.5394  Epoch 205: Loss = 0.5240, Accuracy = 0.6965, Val_Loss = 0.6420, Val_Accuracy = 0.6696\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6650 - loss: 0.5328 - val_accuracy: 0.6696 - val_loss: 0.6420\n",
      "Epoch 206/240\n",
      "\u001b[1m22/34\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6689 - loss: 0.5233Epoch 206: Loss = 0.5143, Accuracy = 0.7132, Val_Loss = 0.6568, Val_Accuracy = 0.6261\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6839 - loss: 0.5226 - val_accuracy: 0.6261 - val_loss: 0.6568\n",
      "Epoch 207/240\n",
      "\u001b[1m25/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7273 - loss: 0.5024Epoch 207: Loss = 0.5056, Accuracy = 0.7188, Val_Loss = 0.6042, Val_Accuracy = 0.7217\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7241 - loss: 0.5034 - val_accuracy: 0.7217 - val_loss: 0.6042\n",
      "Epoch 208/240\n",
      "\u001b[1m23/34\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7421 - loss: 0.4540Epoch 208: Loss = 0.5041, Accuracy = 0.7263, Val_Loss = 0.6588, Val_Accuracy = 0.6261\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7390 - loss: 0.4667 - val_accuracy: 0.6261 - val_loss: 0.6588\n",
      "Epoch 209/240\n",
      "\u001b[1m23/34\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7132 - loss: 0.4890Epoch 209: Loss = 0.5137, Accuracy = 0.7039, Val_Loss = 0.5996, Val_Accuracy = 0.7391\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7124 - loss: 0.4962 - val_accuracy: 0.7391 - val_loss: 0.5996\n",
      "Epoch 210/240\n",
      "\u001b[1m22/34\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7760 - loss: 0.4894Epoch 210: Loss = 0.5101, Accuracy = 0.7225, Val_Loss = 0.6576, Val_Accuracy = 0.6174\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7599 - loss: 0.4946 - val_accuracy: 0.6174 - val_loss: 0.6576\n",
      "Epoch 211/240\n",
      "\u001b[1m27/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6932 - loss: 0.5081Epoch 211: Loss = 0.5096, Accuracy = 0.7300, Val_Loss = 0.6298, Val_Accuracy = 0.6609\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7011 - loss: 0.5083 - val_accuracy: 0.6609 - val_loss: 0.6298\n",
      "Epoch 212/240\n",
      "\u001b[1m29/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7265 - loss: 0.5049Epoch 212: Loss = 0.5122, Accuracy = 0.6890, Val_Loss = 0.6072, Val_Accuracy = 0.7217\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7210 - loss: 0.5057 - val_accuracy: 0.7217 - val_loss: 0.6072\n",
      "Epoch 213/240\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7630 - loss: 0.5175Epoch 213: Loss = 0.5108, Accuracy = 0.7393, Val_Loss = 0.6372, Val_Accuracy = 0.6783\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7623 - loss: 0.5173 - val_accuracy: 0.6783 - val_loss: 0.6372\n",
      "Epoch 214/240\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7100 - loss: 0.5157Epoch 214: Loss = 0.5136, Accuracy = 0.7151, Val_Loss = 0.6232, Val_Accuracy = 0.7043\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7102 - loss: 0.5156 - val_accuracy: 0.7043 - val_loss: 0.6232\n",
      "Epoch 215/240\n",
      "\u001b[1m33/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6999 - loss: 0.5076Epoch 215: Loss = 0.5118, Accuracy = 0.7095, Val_Loss = 0.6708, Val_Accuracy = 0.6000\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7005 - loss: 0.5078 - val_accuracy: 0.6000 - val_loss: 0.6708\n",
      "Epoch 216/240\n",
      "\u001b[1m29/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6887 - loss: 0.4904Epoch 216: Loss = 0.5116, Accuracy = 0.7114, Val_Loss = 0.6435, Val_Accuracy = 0.6522\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6931 - loss: 0.4929 - val_accuracy: 0.6522 - val_loss: 0.6435\n",
      "Epoch 217/240\n",
      "\u001b[1m19/34\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6944 - loss: 0.4906 Epoch 217: Loss = 0.5145, Accuracy = 0.7076, Val_Loss = 0.7002, Val_Accuracy = 0.6000\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6995 - loss: 0.4995 - val_accuracy: 0.6000 - val_loss: 0.7002\n",
      "Epoch 218/240\n",
      "\u001b[1m22/34\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6593 - loss: 0.5352 Epoch 218: Loss = 0.5243, Accuracy = 0.7076, Val_Loss = 0.6303, Val_Accuracy = 0.6609\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6781 - loss: 0.5324 - val_accuracy: 0.6609 - val_loss: 0.6303\n",
      "Epoch 219/240\n",
      "\u001b[1m23/34\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6862 - loss: 0.5334Epoch 219: Loss = 0.5024, Accuracy = 0.7114, Val_Loss = 0.5993, Val_Accuracy = 0.7478\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6993 - loss: 0.5243 - val_accuracy: 0.7478 - val_loss: 0.5993\n",
      "Epoch 220/240\n",
      "\u001b[1m26/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6805 - loss: 0.5503 Epoch 220: Loss = 0.5098, Accuracy = 0.7188, Val_Loss = 0.6102, Val_Accuracy = 0.7217\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6878 - loss: 0.5422 - val_accuracy: 0.7217 - val_loss: 0.6102\n",
      "Epoch 221/240\n",
      "\u001b[1m18/34\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7350 - loss: 0.4785 Epoch 221: Loss = 0.5043, Accuracy = 0.7039, Val_Loss = 0.5914, Val_Accuracy = 0.7478\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7160 - loss: 0.4936 - val_accuracy: 0.7478 - val_loss: 0.5914\n",
      "Epoch 222/240\n",
      "\u001b[1m17/34\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7636 - loss: 0.4630  Epoch 222: Loss = 0.5024, Accuracy = 0.7132, Val_Loss = 0.6008, Val_Accuracy = 0.7304\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7299 - loss: 0.4881 - val_accuracy: 0.7304 - val_loss: 0.6008\n",
      "Epoch 223/240\n",
      "\u001b[1m29/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7054 - loss: 0.5207Epoch 223: Loss = 0.5017, Accuracy = 0.7114, Val_Loss = 0.6074, Val_Accuracy = 0.7304\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7065 - loss: 0.5175 - val_accuracy: 0.7304 - val_loss: 0.6074\n",
      "Epoch 224/240\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6975 - loss: 0.4993Epoch 224: Loss = 0.4993, Accuracy = 0.7132, Val_Loss = 0.6207, Val_Accuracy = 0.7130\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6979 - loss: 0.4993 - val_accuracy: 0.7130 - val_loss: 0.6207\n",
      "Epoch 225/240\n",
      "\u001b[1m33/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6999 - loss: 0.5196Epoch 225: Loss = 0.5088, Accuracy = 0.7114, Val_Loss = 0.6758, Val_Accuracy = 0.6087\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7005 - loss: 0.5190 - val_accuracy: 0.6087 - val_loss: 0.6758\n",
      "Epoch 226/240\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6874 - loss: 0.4937Epoch 226: Loss = 0.5014, Accuracy = 0.7020, Val_Loss = 0.6261, Val_Accuracy = 0.6870\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6878 - loss: 0.4939 - val_accuracy: 0.6870 - val_loss: 0.6261\n",
      "Epoch 227/240\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6617 - loss: 0.5144Epoch 227: Loss = 0.5073, Accuracy = 0.6983, Val_Loss = 0.6551, Val_Accuracy = 0.6435\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6627 - loss: 0.5142 - val_accuracy: 0.6435 - val_loss: 0.6551\n",
      "Epoch 228/240\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6826 - loss: 0.5067Epoch 228: Loss = 0.4988, Accuracy = 0.7039, Val_Loss = 0.6028, Val_Accuracy = 0.7391\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6832 - loss: 0.5065 - val_accuracy: 0.7391 - val_loss: 0.6028\n",
      "Epoch 229/240\n",
      "\u001b[1m26/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7097 - loss: 0.5303Epoch 229: Loss = 0.5051, Accuracy = 0.6946, Val_Loss = 0.6209, Val_Accuracy = 0.7130\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7070 - loss: 0.5227 - val_accuracy: 0.7130 - val_loss: 0.6209\n",
      "Epoch 230/240\n",
      "\u001b[1m31/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7517 - loss: 0.4784Epoch 230: Loss = 0.5022, Accuracy = 0.7244, Val_Loss = 0.6113, Val_Accuracy = 0.7391\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7484 - loss: 0.4810 - val_accuracy: 0.7391 - val_loss: 0.6113\n",
      "Epoch 231/240\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6881 - loss: 0.5182Epoch 231: Loss = 0.5028, Accuracy = 0.7095, Val_Loss = 0.6576, Val_Accuracy = 0.6174\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6887 - loss: 0.5178 - val_accuracy: 0.6174 - val_loss: 0.6576\n",
      "Epoch 232/240\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6985 - loss: 0.4926Epoch 232: Loss = 0.4990, Accuracy = 0.7076, Val_Loss = 0.5952, Val_Accuracy = 0.7304\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6987 - loss: 0.4927 - val_accuracy: 0.7304 - val_loss: 0.5952\n",
      "Epoch 233/240\n",
      "\u001b[1m31/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6958 - loss: 0.5353Epoch 233: Loss = 0.5242, Accuracy = 0.7151, Val_Loss = 0.5887, Val_Accuracy = 0.7217\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6978 - loss: 0.5345 - val_accuracy: 0.7217 - val_loss: 0.5887\n",
      "Epoch 234/240\n",
      "\u001b[1m25/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7417 - loss: 0.5374Epoch 234: Loss = 0.5051, Accuracy = 0.7393, Val_Loss = 0.6568, Val_Accuracy = 0.6609\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7418 - loss: 0.5288 - val_accuracy: 0.6609 - val_loss: 0.6568\n",
      "Epoch 235/240\n",
      "\u001b[1m25/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6956 - loss: 0.4932Epoch 235: Loss = 0.4982, Accuracy = 0.7114, Val_Loss = 0.6546, Val_Accuracy = 0.6522\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7031 - loss: 0.4915 - val_accuracy: 0.6522 - val_loss: 0.6546\n",
      "Epoch 236/240\n",
      "\u001b[1m27/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7300 - loss: 0.4824Epoch 236: Loss = 0.5012, Accuracy = 0.7076, Val_Loss = 0.6146, Val_Accuracy = 0.7217\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7268 - loss: 0.4850 - val_accuracy: 0.7217 - val_loss: 0.6146\n",
      "Epoch 237/240\n",
      "\u001b[1m22/34\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7802 - loss: 0.4383Epoch 237: Loss = 0.4945, Accuracy = 0.7263, Val_Loss = 0.6359, Val_Accuracy = 0.7130\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7586 - loss: 0.4615 - val_accuracy: 0.7130 - val_loss: 0.6359\n",
      "Epoch 238/240\n",
      "\u001b[1m18/34\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7481 - loss: 0.4870  Epoch 238: Loss = 0.5129, Accuracy = 0.7076, Val_Loss = 0.6541, Val_Accuracy = 0.5913\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7330 - loss: 0.4986 - val_accuracy: 0.5913 - val_loss: 0.6541\n",
      "Epoch 239/240\n",
      "\u001b[1m26/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7206 - loss: 0.4875Epoch 239: Loss = 0.5107, Accuracy = 0.7281, Val_Loss = 0.6426, Val_Accuracy = 0.6261\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7238 - loss: 0.4930 - val_accuracy: 0.6261 - val_loss: 0.6426\n",
      "Epoch 240/240\n",
      "\u001b[1m25/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7230 - loss: 0.5086Epoch 240: Loss = 0.4996, Accuracy = 0.7263, Val_Loss = 0.6248, Val_Accuracy = 0.6957\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7265 - loss: 0.5053 - val_accuracy: 0.6957 - val_loss: 0.6248\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "# Asegúrate de que X_train, y_train, X_val, y_val están definidos\n",
    "# Preparar los datos de entrada y salida\n",
    "X_train_keras = np.array(X_train)\n",
    "y_train_keras = np.array(y_train)\n",
    "X_val_keras = np.array(X_val)\n",
    "y_val_keras = np.array(y_val)\n",
    "\n",
    "# Calcular la ponderación de clases\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_keras), y=y_train_keras)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# Definir el modelo de red neuronal en Keras\n",
    "model = Sequential([\n",
    "    Dense(16, input_shape=(X_train_keras.shape[1],), activation='relu'),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), \n",
    "              loss=BinaryCrossentropy(), \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Definir una función de callback para mostrar la pérdida y la precisión en cada época\n",
    "class PrintEpochStats(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f\"Epoch {epoch+1}: Loss = {logs['loss']:.4f}, Accuracy = {logs['accuracy']:.4f}, \"\n",
    "              f\"Val_Loss = {logs['val_loss']:.4f}, Val_Accuracy = {logs['val_accuracy']:.4f}\")\n",
    "\n",
    "# Entrenar el modelo con ponderación de clases y mostrar el progreso en cada época\n",
    "history = model.fit(X_train_keras, y_train_keras, \n",
    "                    epochs=240, \n",
    "                    batch_size=16, \n",
    "                    validation_data=(X_val_keras, y_val_keras), \n",
    "                    class_weight=class_weight_dict, \n",
    "                    callbacks=[PrintEpochStats()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f486637f-5432-45d9-9ec2-46881f200c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dereck.marin\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario: {'layers': [32, 16, 8], 'dropout_rate': 0.3, 'learning_rate': 0.001, 'optimizer_type': 'Adam'}\n",
      "Test Accuracy: 0.3534\n",
      "Test Loss: 0.7605\n",
      "\n",
      "Scenario: {'layers': [64, 32], 'dropout_rate': 0.2, 'learning_rate': 0.0005, 'optimizer_type': 'RMSprop'}\n",
      "Test Accuracy: 0.6897\n",
      "Test Loss: 0.7835\n",
      "\n",
      "Scenario: {'layers': [16, 8], 'dropout_rate': 0.1, 'learning_rate': 0.001, 'optimizer_type': 'Adam'}\n",
      "Test Accuracy: 0.3793\n",
      "Test Loss: 1.3083\n",
      "\n",
      "Scenario: {'layers': [32, 16, 8], 'dropout_rate': 0.4, 'learning_rate': 0.001, 'optimizer_type': 'RMSprop'}\n",
      "Test Accuracy: 0.6638\n",
      "Test Loss: 0.6681\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Preparar los datos de entrada y salida para Keras\n",
    "X_train_keras = np.array(X_train)\n",
    "y_train_keras = np.array(y_train)\n",
    "X_val_keras = np.array(X_val)\n",
    "y_val_keras = np.array(y_val)\n",
    "X_test_keras = np.array(X_test)\n",
    "y_test_keras = np.array(y_test)\n",
    "\n",
    "# Calcular la ponderación de clases\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_keras), y=y_train_keras)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# Definir una función para crear y compilar el modelo\n",
    "def create_model(layers=[16, 8], dropout_rate=0.2, learning_rate=0.001, optimizer_type='Adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(layers[0], input_shape=(X_train_keras.shape[1],), activation='relu'))\n",
    "    model.add(Dropout(dropout_rate))  # Aplicar dropout a la primera capa oculta\n",
    "    for units in layers[1:]:\n",
    "        model.add(Dense(units, activation='relu'))\n",
    "        model.add(Dropout(dropout_rate))  # Aplicar dropout a cada capa oculta adicional\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Elegir el optimizador\n",
    "    if optimizer_type == 'Adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_type == 'RMSprop':\n",
    "        optimizer = RMSprop(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    # Compilar el modelo\n",
    "    model.compile(optimizer=optimizer, loss=BinaryCrossentropy(), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Escenarios para probar\n",
    "scenarios = [\n",
    "    {'layers': [32, 16, 8], 'dropout_rate': 0.3, 'learning_rate': 0.001, 'optimizer_type': 'Adam'},\n",
    "    {'layers': [64, 32], 'dropout_rate': 0.2, 'learning_rate': 0.0005, 'optimizer_type': 'RMSprop'},\n",
    "    {'layers': [16, 8], 'dropout_rate': 0.1, 'learning_rate': 0.001, 'optimizer_type': 'Adam'},\n",
    "    {'layers': [32, 16, 8], 'dropout_rate': 0.4, 'learning_rate': 0.001, 'optimizer_type': 'RMSprop'}\n",
    "]\n",
    "\n",
    "# Variable para almacenar resultados\n",
    "results = []\n",
    "\n",
    "# Configurar parada temprana\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Entrenar y evaluar modelos para cada escenario\n",
    "for scenario in scenarios:\n",
    "    model = create_model(**scenario)\n",
    "    history = model.fit(\n",
    "        X_train_keras, y_train_keras,\n",
    "        epochs=100,\n",
    "        batch_size=16,\n",
    "        validation_data=(X_val_keras, y_val_keras),\n",
    "        class_weight=class_weight_dict,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Evaluar en el conjunto de prueba\n",
    "    test_loss, test_accuracy = model.evaluate(X_test_keras, y_test_keras, verbose=0)\n",
    "    results.append({\n",
    "        'scenario': scenario,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'test_loss': test_loss\n",
    "    })\n",
    "\n",
    "# Mostrar los resultados de cada escenario\n",
    "for result in results:\n",
    "    print(f\"Scenario: {result['scenario']}\")\n",
    "    print(f\"Test Accuracy: {result['test_accuracy']:.4f}\")\n",
    "    print(f\"Test Loss: {result['test_loss']:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2ac7af8d-0082-4f04-b2a7-b0945203a2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario: {'layers': [32, 16, 8], 'dropout_rate': 0.3, 'learning_rate': 0.001, 'optimizer_type': 'Adam'}\n",
      "Test Accuracy: 0.7500\n",
      "Test Loss: 0.6225\n",
      "\n",
      "Scenario: {'layers': [64, 32], 'dropout_rate': 0.2, 'learning_rate': 0.0005, 'optimizer_type': 'RMSprop'}\n",
      "Test Accuracy: 0.4138\n",
      "Test Loss: 1.3817\n",
      "\n",
      "Scenario: {'layers': [16, 8], 'dropout_rate': 0.1, 'learning_rate': 0.001, 'optimizer_type': 'Adam'}\n",
      "Test Accuracy: 0.3534\n",
      "Test Loss: 2.0695\n",
      "\n",
      "Scenario: {'layers': [32, 16, 8], 'dropout_rate': 0.4, 'learning_rate': 0.001, 'optimizer_type': 'RMSprop'}\n",
      "Test Accuracy: 0.3190\n",
      "Test Loss: 0.8647\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization, Input\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Nadam, Adam\n",
    "\n",
    "def create_model(layers=[16, 8], dropout_rate=0.2, l2_reg=0.001, learning_rate=0.001, optimizer_type='Adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(X_train_keras.shape[1],)))  # Definir la forma de entrada correctamente\n",
    "    model.add(Dense(layers[0], activation='relu', kernel_regularizer=l2(l2_reg)))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    for units in layers[1:]:\n",
    "        model.add(Dense(units, activation='relu', kernel_regularizer=l2(l2_reg)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Configuración del optimizador\n",
    "    if optimizer_type == 'Adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_type == 'Nadam':\n",
    "        optimizer = Nadam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = Adam(learning_rate=learning_rate)  # Usar Adam como predeterminado\n",
    "    \n",
    "    # Compilar el modelo\n",
    "    model.compile(optimizer=optimizer, loss=BinaryCrossentropy(), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Configuración de los callbacks avanzados\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=15, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "# Entrenamiento y evaluación\n",
    "results = []\n",
    "for scenario in scenarios:\n",
    "    model = create_model(**scenario)\n",
    "    history = model.fit(\n",
    "        X_train_keras, y_train_keras,\n",
    "        epochs=200,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_val_keras, y_val_keras),\n",
    "        class_weight=class_weight_dict,\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Evaluación en el conjunto de prueba\n",
    "    test_loss, test_accuracy = model.evaluate(X_test_keras, y_test_keras, verbose=0)\n",
    "    results.append({\n",
    "        'scenario': scenario,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'test_loss': test_loss\n",
    "    })\n",
    "\n",
    "# Mostrar resultados\n",
    "for result in results:\n",
    "    print(f\"Scenario: {result['scenario']}\")\n",
    "    print(f\"Test Accuracy: {result['test_accuracy']:.4f}\")\n",
    "    print(f\"Test Loss: {result['test_loss']:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb75db44-508b-4b82-a4e1-1be4fc1c9ba6",
   "metadata": {},
   "source": [
    "# Regresion Logistica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "81292af8-802b-475c-b77e-9dd8b651c280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7931\n",
      "Confusion Matrix:\n",
      "[[64 11]\n",
      " [13 28]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.85      0.84        75\n",
      "           1       0.72      0.68      0.70        41\n",
      "\n",
      "    accuracy                           0.79       116\n",
      "   macro avg       0.77      0.77      0.77       116\n",
      "weighted avg       0.79      0.79      0.79       116\n",
      "\n",
      "AUC Score: 0.8598\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Entrenamiento del modelo de regresión logística\n",
    "log_reg = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "log_reg.fit(X_train_keras, y_train_keras)\n",
    "\n",
    "# Predicciones en el conjunto de prueba\n",
    "y_pred = log_reg.predict(X_test_keras)\n",
    "y_pred_proba = log_reg.predict_proba(X_test_keras)[:, 1]  # Probabilidades para calcular AUC\n",
    "\n",
    "# Evaluación de precisión\n",
    "test_accuracy = accuracy_score(y_test_keras, y_pred)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Matriz de confusión y reporte de clasificación\n",
    "conf_matrix = confusion_matrix(y_test_keras, y_pred)\n",
    "class_report = classification_report(y_test_keras, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "# Curva ROC y AUC\n",
    "roc_auc = roc_auc_score(y_test_keras, y_pred_proba)\n",
    "fpr, tpr, _ = roc_curve(y_test_keras, y_pred_proba)\n",
    "print(f\"AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "# Graficar la curva ROC\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', label=f'ROC Curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "045ae126-a8d9-4759-9eb6-f411eaf883ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado como 'logistic_regression_model.pkl'\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 1. Entrenar el modelo (usa tus datos de entrenamiento)\n",
    "log_reg = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "log_reg.fit(X_train_keras, y_train_keras)\n",
    "\n",
    "# 2. Guardar el modelo entrenado en un archivo\n",
    "model_filename = 'logistic_regression_model.pkl'\n",
    "joblib.dump(log_reg, model_filename)\n",
    "print(f\"Modelo guardado como '{model_filename}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c832942c-4fd6-45d0-97f5-369aaa9ad8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Función para cargar el modelo y realizar una predicción descriptiva\n",
    "def predict_with_logistic_regression(input_data):\n",
    "    \"\"\"\n",
    "    Predice si una persona tiene diabetes usando el modelo de regresión logística guardado.\n",
    "    \n",
    "    Parámetros:\n",
    "        input_data (list o array): Una lista o array de longitud 8 con los valores de las características de entrada.\n",
    "    \n",
    "    Retorno:\n",
    "        resultado (str): \"Tiene diabetes\" o \"No tiene diabetes\" basado en la predicción del modelo.\n",
    "    \"\"\"\n",
    "    # Verificar que input_data tenga la longitud correcta\n",
    "    if len(input_data) != 8:\n",
    "        raise ValueError(\"La entrada debe ser una lista o array de 8 elementos.\")\n",
    "    \n",
    "    # Cargar el modelo guardado\n",
    "    log_reg = joblib.load('logistic_regression_model.pkl')\n",
    "    \n",
    "    # Convertir la entrada a un array de numpy y remodelar para predecir\n",
    "    input_array = np.array(input_data).reshape(1, -1)\n",
    "    prediction = log_reg.predict(input_array)\n",
    "    \n",
    "    # Interpretar el resultado de la predicción\n",
    "    if prediction[0] == 1:\n",
    "        return \"Tiene diabetes\"\n",
    "    else:\n",
    "        return \"No tiene diabetes\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a0c25f98-9250-4949-b7cf-67cc8572cc8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultado: No tiene diabetes\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de parámetros de entrada (reemplaza estos valores con los reales)\n",
    "input_data = [0.5, -1.2, 0.3, 1.0, -0.5, 1.2, -0.7, 0.9]\n",
    "\n",
    "# Llamar a la función para obtener una predicción\n",
    "resultado = predict_with_logistic_regression(input_data)\n",
    "print(f\"Resultado: {resultado}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ba7c0e5d-7ffc-40af-a476-9dffaaf23e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultado para el conjunto de datos 1: No tiene diabetes\n",
      "Resultado para el conjunto de datos 2: No tiene diabetes\n",
      "Resultado para el conjunto de datos 3: No tiene diabetes\n",
      "Resultado para el conjunto de datos 4: Tiene diabetes\n",
      "Resultado para el conjunto de datos 5: Tiene diabetes\n"
     ]
    }
   ],
   "source": [
    "# Lista de datos de prueba\n",
    "test_data = [\n",
    "    [1, 85, 66, 29, 0, 26.6, 0.351, 31],\n",
    "    [1, 89, 66, 23, 94, 28.1, 0.167, 21],\n",
    "    [5, 116, 74, 0, 0, 25.6, 0.201, 30],\n",
    "    [10, 115, 0, 0, 0, 35.3, 0.134, 29],\n",
    "    [4, 110, 92, 0, 0, 37.6, 0.191, 30]\n",
    "]\n",
    "\n",
    "# Probar cada entrada y mostrar el resultado\n",
    "for i, data in enumerate(test_data, start=1):\n",
    "    resultado = predict_with_logistic_regression(data)\n",
    "    print(f\"Resultado para el conjunto de datos {i}: {resultado}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f88ffda0-e695-45a4-93af-f27f297020a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultado para el conjunto de datos 1: Tiene diabetes\n",
      "Resultado para el conjunto de datos 2: Tiene diabetes\n",
      "Resultado para el conjunto de datos 3: Tiene diabetes\n",
      "Resultado para el conjunto de datos 4: No tiene diabetes\n",
      "Resultado para el conjunto de datos 5: Tiene diabetes\n",
      "Resultado para el conjunto de datos 6: No tiene diabetes\n",
      "Resultado para el conjunto de datos 7: Tiene diabetes\n",
      "Resultado para el conjunto de datos 8: Tiene diabetes\n",
      "Resultado para el conjunto de datos 9: Tiene diabetes\n",
      "Resultado para el conjunto de datos 10: No tiene diabetes\n"
     ]
    }
   ],
   "source": [
    "# Nuevos datos de prueba con casos positivos esperados\n",
    "test_data = [\n",
    "    [6, 148, 72, 35, 0, 33.6, 0.627, 50],\n",
    "    [8, 183, 64, 0, 0, 23.3, 0.672, 32],\n",
    "    [0, 137, 40, 35, 168, 43.1, 2.288, 33],\n",
    "    [3, 78, 50, 32, 88, 31, 0.248, 26],\n",
    "    [2, 197, 70, 45, 543, 30.5, 0.158, 53],\n",
    "    [8, 125, 96, 0, 0, 0, 0.232, 54],\n",
    "    [10, 168, 74, 0, 0, 38, 0.537, 34],\n",
    "    [1, 189, 60, 23, 846, 30.1, 0.398, 59],\n",
    "    [5, 166, 72, 19, 175, 25.8, 0.587, 51],\n",
    "    [7, 100, 0, 0, 0, 30, 0.484, 32]\n",
    "]\n",
    "\n",
    "# Probar cada entrada con un umbral ajustado y mostrar el resultado\n",
    "for i, data in enumerate(test_data, start=1):\n",
    "    resultado = predict_with_logistic_regression(data)  # Se usa threshold=0.4 para aumentar sensibilidad\n",
    "    print(f\"Resultado para el conjunto de datos {i}: {resultado}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "94de30cf-f317-4d7c-b13d-9bddfdec368f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Función para cargar el modelo y realizar una predicción descriptiva con ajuste de umbral\n",
    "def predict_with_logistic_regression(input_data, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Predice si una persona tiene diabetes usando el modelo de regresión logística guardado.\n",
    "    \n",
    "    Parámetros:\n",
    "        input_data (list o array): Una lista o array de longitud 8 con los valores de las características de entrada.\n",
    "        threshold (float): Umbral para clasificar como \"Tiene diabetes\". Por defecto es 0.5.\n",
    "    \n",
    "    Retorno:\n",
    "        resultado (str): \"Tiene diabetes\" o \"No tiene diabetes\" basado en la predicción del modelo.\n",
    "    \"\"\"\n",
    "    # Verificar que input_data tenga la longitud correcta\n",
    "    if len(input_data) != 8:\n",
    "        raise ValueError(\"La entrada debe ser una lista o array de 8 elementos.\")\n",
    "    \n",
    "    # Cargar el modelo guardado\n",
    "    log_reg = joblib.load('logistic_regression_model.pkl')\n",
    "    \n",
    "    # Convertir la entrada a un array de numpy y remodelar para predecir la probabilidad\n",
    "    input_array = np.array(input_data).reshape(1, -1)\n",
    "    probability = log_reg.predict_proba(input_array)[0, 1]  # Probabilidad de la clase \"Tiene diabetes\"\n",
    "    \n",
    "    # Clasificar en base al umbral ajustado\n",
    "    if probability >= threshold:\n",
    "        return \"Tiene diabetes\"\n",
    "    else:\n",
    "        return \"No tiene diabetes\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b4362aa2-e6fc-473f-8ec7-ce044f55e8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultado para el conjunto de datos 1: Tiene diabetes\n",
      "Resultado para el conjunto de datos 2: Tiene diabetes\n",
      "Resultado para el conjunto de datos 3: Tiene diabetes\n",
      "Resultado para el conjunto de datos 4: No tiene diabetes\n",
      "Resultado para el conjunto de datos 5: Tiene diabetes\n",
      "Resultado para el conjunto de datos 6: No tiene diabetes\n",
      "Resultado para el conjunto de datos 7: Tiene diabetes\n",
      "Resultado para el conjunto de datos 8: Tiene diabetes\n",
      "Resultado para el conjunto de datos 9: Tiene diabetes\n",
      "Resultado para el conjunto de datos 10: No tiene diabetes\n"
     ]
    }
   ],
   "source": [
    "# Nuevos datos de prueba con casos positivos esperados\n",
    "test_data = [\n",
    "    [6, 148, 72, 35, 0, 33.6, 0.627, 50],\n",
    "    [8, 183, 64, 0, 0, 23.3, 0.672, 32],\n",
    "    [0, 137, 40, 35, 168, 43.1, 2.288, 33],\n",
    "    [3, 78, 50, 32, 88, 31, 0.248, 26],\n",
    "    [2, 197, 70, 45, 543, 30.5, 0.158, 53],\n",
    "    [8, 125, 96, 0, 0, 0, 0.232, 54],\n",
    "    [10, 168, 74, 0, 0, 38, 0.537, 34],\n",
    "    [1, 189, 60, 23, 846, 30.1, 0.398, 59],\n",
    "    [5, 166, 72, 19, 175, 25.8, 0.587, 51],\n",
    "    [7, 100, 0, 0, 0, 30, 0.484, 32]\n",
    "]\n",
    "\n",
    "# Probar cada entrada con un umbral ajustado y mostrar el resultado\n",
    "for i, data in enumerate(test_data, start=1):\n",
    "    resultado = predict_with_logistic_regression(data)  # Se usa threshold=0.4 para aumentar sensibilidad\n",
    "    print(f\"Resultado para el conjunto de datos {i}: {resultado}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "fde18354-cb13-4d3b-a417-fe2a46a5cce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultado para el conjunto de datos 1: No tiene diabetes\n",
      "Resultado para el conjunto de datos 2: No tiene diabetes\n",
      "Resultado para el conjunto de datos 3: No tiene diabetes\n",
      "Resultado para el conjunto de datos 4: Tiene diabetes\n",
      "Resultado para el conjunto de datos 5: Tiene diabetes\n"
     ]
    }
   ],
   "source": [
    "# Lista de datos de prueba\n",
    "test_data = [\n",
    "    [1, 85, 66, 29, 0, 26.6, 0.351, 31],\n",
    "    [1, 89, 66, 23, 94, 28.1, 0.167, 21],\n",
    "    [5, 116, 74, 0, 0, 25.6, 0.201, 30],\n",
    "    [10, 115, 0, 0, 0, 35.3, 0.134, 29],\n",
    "    [4, 110, 92, 0, 0, 37.6, 0.191, 30]\n",
    "]\n",
    "\n",
    "# Probar cada entrada y mostrar el resultado\n",
    "for i, data in enumerate(test_data, start=1):\n",
    "    resultado = predict_with_logistic_regression(data)\n",
    "    print(f\"Resultado para el conjunto de datos {i}: {resultado}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97d46cdf-244f-4c6e-b74f-1cb2b217b84e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler.pkl']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Cargar los datos de entrenamiento desde el archivo local\n",
    "training_data = pd.read_csv(\"diabetes.csv\", encoding='utf-8')\n",
    "\n",
    "# Seleccionar las columnas de características (excluyendo el objetivo 'Outcome')\n",
    "features = training_data[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', \n",
    "                          'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']]\n",
    "\n",
    "# Crear y ajustar el imputador y escalador\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Ajustar el imputador y luego el escalador\n",
    "imputed_data = imputer.fit_transform(features)  # Ajustar y transformar con el imputador\n",
    "scaler.fit(imputed_data)  # Ajustar el escalador usando los datos imputados\n",
    "\n",
    "# Guardar los preprocesadores ajustados en archivos\n",
    "joblib.dump(imputer, \"imputer.pkl\")\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
